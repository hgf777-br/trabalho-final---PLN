{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDbi6PDS9MYO"
   },
   "source": [
    "***Participantes (RM - NOME):***<br>\n",
    "339708 - Roberto<br>\n",
    "340192 - Sergio<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xw6WhaNo4k3"
   },
   "source": [
    "## **Criar um classificador de sentimento aplicando técnicas de PLN**\n",
    "---\n",
    "\n",
    "Utilizando o dataset de revisões de filmes em português [1], criar um classificador de sentimentos que consiga um score na métrica F1 Score superior a 70%.\n",
    "\n",
    "Devem utilizar uma amostra de 20% e randon_state igual a 42 para testar as implementações e mensurar a métrica F1 Score (usar o parâmetro average = 'weighted') o restante dos dados devem ser utilizados para o treinamento (80%).\n",
    "\n",
    "Fique a vontade para testar os métodos de pré-processamento, abordagens, algoritmos e bibliotecas, mas explique e justifique suas decisões.\n",
    "O trabalho poderá ser feito em grupo de até 4 pessoas (mesmo grupo do Startup One).\n",
    "\n",
    "Separe a implementação do seu modelo campeão junto com a parte de validação/teste de forma que o professor consiga executar todo o pipeline do modelo campeão.\n",
    "\n",
    "Composição da nota:\n",
    "- 50% - Demonstrações das aplicações das técnicas de PLN (regras, pré-processamentos, tratamentos, variedade de modelos aplicados, etc.)\n",
    "- 50% - Baseado na performance obtida com o dataset de teste (conforme recomendação da amostra) no seu modelo campeão e na validação que o professor processar (Métrica F1 Score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzhQpodBpRpX"
   },
   "source": [
    "[1] - https://dados-ml-pln.s3-sa-east-1.amazonaws.com/reviews-pt-br.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Roberto\n",
      "[nltk_data]     Tengan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import seaborn as sb\n",
    "\n",
    "from nltk.stem.rslp import RSLPStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "rslp = RSLPStemmer()\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função de lematização completa do documento\n",
    "def stemmer(frase):\n",
    "  tokens = [rslp.stem(w) for w in frase.split()]\n",
    "  return \" \".join(tokens)\n",
    "\n",
    "# função de lematização completa do documento\n",
    "def lemmatizer(frase):\n",
    "  doc = nlp(frase)\n",
    "  tokens = [w.lemma_ for w in doc]\n",
    "  return \" \".join(tokens)\n",
    "\n",
    "# função de lematização para os verbos do documento\n",
    "def lemmatizer_verbs(frase):\n",
    "  doc = nlp(frase)\n",
    "  tokens = [w.lemma_ if w.pos_ == 'VERB' else w.text for w in doc]\n",
    "  return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DMBI8SQtps1n"
   },
   "outputs": [],
   "source": [
    "stops = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
    "df = pd.read_csv('./data/reviews-pt-br.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s__lBzDQwrcG",
    "outputId": "0bd3d84d-be60-4da3-c598-62f4a045b6c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44514 entries, 0 to 44513\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   codigo      44514 non-null  int64 \n",
      " 1   texto       44514 non-null  object\n",
      " 2   sentimento  44514 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyKC9Vhkp0BK"
   },
   "source": [
    "Conferindo se temos dados nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nze8UbKhosm9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "codigo        0\n",
       "texto         0\n",
       "sentimento    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ue0nV0uVo3OZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codigo</th>\n",
       "      <th>texto</th>\n",
       "      <th>sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Esse bocejo de pia de cozinha de orçamento mui...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>O Bravo parece indicar que o personagem princi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Durante a Guerra pela Independência do Sul, GE...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>É fora de questão que a verdadeira Anna Anders...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Concordo totalmente com outro dos revisores aq...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Obra-prima absoluta de um filme! Boa noite Mr....</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Embora a palavra megalmania seja muito usada p...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Esta tem que ser a peça mais incrível de porca...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Eu suponho que todas as piadas internas são o ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Se há um tema deste filme, é que as pessoas po...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   codigo                                              texto sentimento\n",
       "0       1  Esse bocejo de pia de cozinha de orçamento mui...        neg\n",
       "1       2  O Bravo parece indicar que o personagem princi...        neg\n",
       "2       3  Durante a Guerra pela Independência do Sul, GE...        pos\n",
       "3       4  É fora de questão que a verdadeira Anna Anders...        pos\n",
       "4       5  Concordo totalmente com outro dos revisores aq...        neg\n",
       "5       6  Obra-prima absoluta de um filme! Boa noite Mr....        pos\n",
       "6       7  Embora a palavra megalmania seja muito usada p...        pos\n",
       "7       8  Esta tem que ser a peça mais incrível de porca...        neg\n",
       "8       9  Eu suponho que todas as piadas internas são o ...        neg\n",
       "9      10  Se há um tema deste filme, é que as pessoas po...        pos"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codigo</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentimento</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>22307</td>\n",
       "      <td>22307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>22207</td>\n",
       "      <td>22207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            codigo  texto\n",
       "sentimento               \n",
       "neg          22307  22307\n",
       "pos          22207  22207"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('sentimento').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FziwgqJmw9OD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sentimento', ylabel='count'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.countplot(x=df.sentimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['stemmer'] = df.texto.apply(stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "df['lemma'] = df.texto.apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['lemma_verb'] = df.texto.apply(lemmatizer_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/criticas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codigo</th>\n",
       "      <th>texto</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>stemmer</th>\n",
       "      <th>lemma</th>\n",
       "      <th>lemma_verb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Esse bocejo de pia de cozinha de orçamento mui...</td>\n",
       "      <td>neg</td>\n",
       "      <td>ess bocej de pia de co de orç muit baix é o ti...</td>\n",
       "      <td>Esse bocejar de pio de cozinhar de orçamentar ...</td>\n",
       "      <td>Esse bocejo de pia de cozinha de orçamento mui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>O Bravo parece indicar que o personagem princi...</td>\n",
       "      <td>neg</td>\n",
       "      <td>o brav parec indic que o person principal, cla...</td>\n",
       "      <td>O Bravo parecer indicar que o personagem princ...</td>\n",
       "      <td>O Bravo parecer indicar que o personagem princ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Durante a Guerra pela Independência do Sul, GE...</td>\n",
       "      <td>pos</td>\n",
       "      <td>dur a guerr pel independ do sul, gener spanky ...</td>\n",
       "      <td>Durante o Guerra pelar Independência do Sul , ...</td>\n",
       "      <td>Durante a Guerra pela Independência do Sul , G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>É fora de questão que a verdadeira Anna Anders...</td>\n",
       "      <td>pos</td>\n",
       "      <td>é for de quest que a verd ann anderson não era...</td>\n",
       "      <td>É ser de questão que o verdadeiro Anna Anderso...</td>\n",
       "      <td>É fora de questão que a verdadeira Anna Anders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Concordo totalmente com outro dos revisores aq...</td>\n",
       "      <td>neg</td>\n",
       "      <td>concord total com outr do revi aqu que fic sat...</td>\n",
       "      <td>Concordo totalmente com outro dos revisor aqui...</td>\n",
       "      <td>Concordo totalmente com outro dos revisores aq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Obra-prima absoluta de um filme! Boa noite Mr....</td>\n",
       "      <td>pos</td>\n",
       "      <td>obra-pr absolut de um filme! boa noit mr.tom r...</td>\n",
       "      <td>Obra-prima absoluto de um filmar ! Boa noite M...</td>\n",
       "      <td>Obra-prima absoluta de um filme ! Boa noite Mr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Embora a palavra megalmania seja muito usada p...</td>\n",
       "      <td>pos</td>\n",
       "      <td>emb a palavr megalman sej muit us par descrev ...</td>\n",
       "      <td>Embora o palavra megalmania ser muito usar par...</td>\n",
       "      <td>Embora a palavra megalmania seja muito usar pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Esta tem que ser a peça mais incrível de porca...</td>\n",
       "      <td>neg</td>\n",
       "      <td>est tem que ser a peç mais incr de porc cinema...</td>\n",
       "      <td>Esta ter que ser o pedir mais incrível de porc...</td>\n",
       "      <td>Esta tem que ser a peça mais incrível de porca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Eu suponho que todas as piadas internas são o ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>eu suponh que tod as pi intern são o que fez d...</td>\n",
       "      <td>Eu supor que todo o piar interno ser o que faz...</td>\n",
       "      <td>Eu supor que todas as piadas internas são o qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Se há um tema deste filme, é que as pessoas po...</td>\n",
       "      <td>pos</td>\n",
       "      <td>se há um tem dest filme, é que as pesso pod li...</td>\n",
       "      <td>Se haver um temer dar filmar , ser que o pesso...</td>\n",
       "      <td>Se haver um tema deste filme , ser que as pess...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   codigo                                              texto sentimento  \\\n",
       "0       1  Esse bocejo de pia de cozinha de orçamento mui...        neg   \n",
       "1       2  O Bravo parece indicar que o personagem princi...        neg   \n",
       "2       3  Durante a Guerra pela Independência do Sul, GE...        pos   \n",
       "3       4  É fora de questão que a verdadeira Anna Anders...        pos   \n",
       "4       5  Concordo totalmente com outro dos revisores aq...        neg   \n",
       "5       6  Obra-prima absoluta de um filme! Boa noite Mr....        pos   \n",
       "6       7  Embora a palavra megalmania seja muito usada p...        pos   \n",
       "7       8  Esta tem que ser a peça mais incrível de porca...        neg   \n",
       "8       9  Eu suponho que todas as piadas internas são o ...        neg   \n",
       "9      10  Se há um tema deste filme, é que as pessoas po...        pos   \n",
       "\n",
       "                                             stemmer  \\\n",
       "0  ess bocej de pia de co de orç muit baix é o ti...   \n",
       "1  o brav parec indic que o person principal, cla...   \n",
       "2  dur a guerr pel independ do sul, gener spanky ...   \n",
       "3  é for de quest que a verd ann anderson não era...   \n",
       "4  concord total com outr do revi aqu que fic sat...   \n",
       "5  obra-pr absolut de um filme! boa noit mr.tom r...   \n",
       "6  emb a palavr megalman sej muit us par descrev ...   \n",
       "7  est tem que ser a peç mais incr de porc cinema...   \n",
       "8  eu suponh que tod as pi intern são o que fez d...   \n",
       "9  se há um tem dest filme, é que as pesso pod li...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  Esse bocejar de pio de cozinhar de orçamentar ...   \n",
       "1  O Bravo parecer indicar que o personagem princ...   \n",
       "2  Durante o Guerra pelar Independência do Sul , ...   \n",
       "3  É ser de questão que o verdadeiro Anna Anderso...   \n",
       "4  Concordo totalmente com outro dos revisor aqui...   \n",
       "5  Obra-prima absoluto de um filmar ! Boa noite M...   \n",
       "6  Embora o palavra megalmania ser muito usar par...   \n",
       "7  Esta ter que ser o pedir mais incrível de porc...   \n",
       "8  Eu supor que todo o piar interno ser o que faz...   \n",
       "9  Se haver um temer dar filmar , ser que o pesso...   \n",
       "\n",
       "                                          lemma_verb  \n",
       "0  Esse bocejo de pia de cozinha de orçamento mui...  \n",
       "1  O Bravo parecer indicar que o personagem princ...  \n",
       "2  Durante a Guerra pela Independência do Sul , G...  \n",
       "3  É fora de questão que a verdadeira Anna Anders...  \n",
       "4  Concordo totalmente com outro dos revisores aq...  \n",
       "5  Obra-prima absoluta de um filme ! Boa noite Mr...  \n",
       "6  Embora a palavra megalmania seja muito usar pa...  \n",
       "7  Esta tem que ser a peça mais incrível de porca...  \n",
       "8  Eu supor que todas as piadas internas são o qu...  \n",
       "9  Se haver um tema deste filme , ser que as pess...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino, df_teste = train_test_split(\n",
    "      df, \n",
    "      test_size = 0.2, \n",
    "      random_state = 42\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Texto Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7139\n",
      "F1 Score: 0.7139\n",
      "=======================================\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8779\n",
      "F1 Score: 0.8779\n",
      "=======================================\n",
      "Wall time: 5.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8425\n",
      "F1 Score: 0.8425\n",
      "=======================================\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8581\n",
      "F1 Score: 0.858\n",
      "=======================================\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8606\n",
      "F1 Score: 0.8606\n",
      "=======================================\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Texto Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6856\n",
      "F1 Score: 0.6856\n",
      "=======================================\n",
      "Wall time: 6min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8814\n",
      "F1 Score: 0.8814\n",
      "=======================================\n",
      "Wall time: 46.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8253\n",
      "F1 Score: 0.8253\n",
      "=======================================\n",
      "Wall time: 8min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8842\n",
      "F1 Score: 0.8841\n",
      "=======================================\n",
      "Wall time: 2.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8837\n",
      "F1 Score: 0.8836\n",
      "=======================================\n",
      "Wall time: 2.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Texto Unigrama - Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.724\n",
      "F1 Score: 0.724\n",
      "=======================================\n",
      "Wall time: 6min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8975\n",
      "F1 Score: 0.8974\n",
      "=======================================\n",
      "Wall time: 55.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8485\n",
      "F1 Score: 0.8485\n",
      "=======================================\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8843\n",
      "F1 Score: 0.8842\n",
      "=======================================\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8814\n",
      "F1 Score: 0.8813\n",
      "=======================================\n",
      "Wall time: 3.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Texto / Unigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1), min_df=10)\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7081\n",
      "F1 Score: 0.7081\n",
      "=======================================\n",
      "Wall time: 54.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8788\n",
      "F1 Score: 0.8788\n",
      "=======================================\n",
      "Wall time: 3.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8387\n",
      "F1 Score: 0.8387\n",
      "=======================================\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8517\n",
      "F1 Score: 0.8517\n",
      "=======================================\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8563\n",
      "F1 Score: 0.8563\n",
      "=======================================\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Texto / Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2), min_df=10)\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6857\n",
      "F1 Score: 0.6857\n",
      "=======================================\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8732\n",
      "F1 Score: 0.8732\n",
      "=======================================\n",
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8217\n",
      "F1 Score: 0.8215\n",
      "=======================================\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8786\n",
      "F1 Score: 0.8786\n",
      "=======================================\n",
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8726\n",
      "F1 Score: 0.8725\n",
      "=======================================\n",
      "Wall time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Texto / Unigrama - Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), min_df=10)\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7155\n",
      "F1 Score: 0.7155\n",
      "=======================================\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8922\n",
      "F1 Score: 0.8922\n",
      "=======================================\n",
      "Wall time: 8.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8516\n",
      "F1 Score: 0.8516\n",
      "=======================================\n",
      "Wall time: 43.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8761\n",
      "F1 Score: 0.8761\n",
      "=======================================\n",
      "Wall time: 2.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.877\n",
      "F1 Score: 0.877\n",
      "=======================================\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Texto / Unigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1), min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7163\n",
      "F1 Score: 0.7163\n",
      "=======================================\n",
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8706\n",
      "F1 Score: 0.8706\n",
      "=======================================\n",
      "Wall time: 3.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8421\n",
      "F1 Score: 0.8421\n",
      "=======================================\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8505\n",
      "F1 Score: 0.8504\n",
      "=======================================\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8541\n",
      "F1 Score: 0.8541\n",
      "=======================================\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Texto / Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2), min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7128\n",
      "F1 Score: 0.7128\n",
      "=======================================\n",
      "Wall time: 48.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8661\n",
      "F1 Score: 0.8661\n",
      "=======================================\n",
      "Wall time: 4.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.824\n",
      "F1 Score: 0.8239\n",
      "=======================================\n",
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8755\n",
      "F1 Score: 0.8755\n",
      "=======================================\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8732\n",
      "F1 Score: 0.8731\n",
      "=======================================\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Texto / Unigrama - Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7276\n",
      "F1 Score: 0.7276\n",
      "=======================================\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8904\n",
      "F1 Score: 0.8904\n",
      "=======================================\n",
      "Wall time: 7.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8469\n",
      "F1 Score: 0.8469\n",
      "=======================================\n",
      "Wall time: 38.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8733\n",
      "F1 Score: 0.8733\n",
      "=======================================\n",
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8755\n",
      "F1 Score: 0.8755\n",
      "=======================================\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Texto / Unigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras / Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['amos', 'atr', 'atrav', 'dever', 'ent', 'est', 'estiv', 'far', 'houv', 'ltimo', 'mero', 'meros', 'nhamos', 'porqu', 'posi', 'poss', 'prio', 'quest', 'ramos', 'rea', 'rela', 'rios', 'ssemos', 'tamb', 'tima', 'timo', 'tiv', 'vamos', 'vel', 'voc', 'xima', 'ximo'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1), min_df=10, token_pattern='[a-zA-Z0-9]{3,}', stop_words=stops)\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7157\n",
      "F1 Score: 0.7157\n",
      "=======================================\n",
      "Wall time: 41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8621\n",
      "F1 Score: 0.8621\n",
      "=======================================\n",
      "Wall time: 2.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8413\n",
      "F1 Score: 0.8413\n",
      "=======================================\n",
      "Wall time: 28.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8516\n",
      "F1 Score: 0.8516\n",
      "=======================================\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8489\n",
      "F1 Score: 0.8489\n",
      "=======================================\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Texto / Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras / Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['amos', 'atr', 'atrav', 'dever', 'ent', 'est', 'estiv', 'far', 'houv', 'ltimo', 'mero', 'meros', 'nhamos', 'porqu', 'posi', 'poss', 'prio', 'quest', 'ramos', 'rea', 'rela', 'rios', 'ssemos', 'tamb', 'tima', 'timo', 'tiv', 'vamos', 'vel', 'voc', 'xima', 'ximo'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2), min_df=10, token_pattern='[a-zA-Z0-9]{3,}', stop_words=stops)\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7126\n",
      "F1 Score: 0.7126\n",
      "=======================================\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8322\n",
      "F1 Score: 0.8322\n",
      "=======================================\n",
      "Wall time: 2.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.7931\n",
      "F1 Score: 0.793\n",
      "=======================================\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.7931\n",
      "F1 Score: 0.793\n",
      "=======================================\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8466\n",
      "F1 Score: 0.8463\n",
      "=======================================\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Texto / Unigrama - Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras / Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['amos', 'atr', 'atrav', 'dever', 'ent', 'est', 'estiv', 'far', 'houv', 'ltimo', 'mero', 'meros', 'nhamos', 'porqu', 'posi', 'poss', 'prio', 'quest', 'ramos', 'rea', 'rela', 'rios', 'ssemos', 'tamb', 'tima', 'timo', 'tiv', 'vamos', 'vel', 'voc', 'xima', 'ximo'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), min_df=10, token_pattern='[a-zA-Z0-9]{3,}', stop_words=stops)\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7214\n",
      "F1 Score: 0.7214\n",
      "=======================================\n",
      "Wall time: 54.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8777\n",
      "F1 Score: 0.8777\n",
      "=======================================\n",
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8497\n",
      "F1 Score: 0.8497\n",
      "=======================================\n",
      "Wall time: 30.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8681\n",
      "F1 Score: 0.8681\n",
      "=======================================\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.869\n",
      "F1 Score: 0.869\n",
      "=======================================\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFA-CYfawkEJ"
   },
   "source": [
    "## CountVectorizer Stemmer Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6983\n",
      "F1 Score: 0.6983\n",
      "=======================================\n",
      "Wall time: 58.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8793\n",
      "F1 Score: 0.8793\n",
      "=======================================\n",
      "Wall time: 5.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8342\n",
      "F1 Score: 0.8342\n",
      "=======================================\n",
      "Wall time: 37.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8476\n",
      "F1 Score: 0.8474\n",
      "=======================================\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8584\n",
      "F1 Score: 0.8583\n",
      "=======================================\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Stemmer Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6865\n",
      "F1 Score: 0.6865\n",
      "=======================================\n",
      "Wall time: 5min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8744\n",
      "F1 Score: 0.8744\n",
      "=======================================\n",
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8219\n",
      "F1 Score: 0.8218\n",
      "=======================================\n",
      "Wall time: 8min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8821\n",
      "F1 Score: 0.8819\n",
      "=======================================\n",
      "Wall time: 2.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8812\n",
      "F1 Score: 0.881\n",
      "=======================================\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Stemmer / Unigrama - Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.702\n",
      "F1 Score: 0.702\n",
      "=======================================\n",
      "Wall time: 5min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8946\n",
      "F1 Score: 0.8946\n",
      "=======================================\n",
      "Wall time: 50.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8445\n",
      "F1 Score: 0.8445\n",
      "=======================================\n",
      "Wall time: 5min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8799\n",
      "F1 Score: 0.8798\n",
      "=======================================\n",
      "Wall time: 3.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8805\n",
      "F1 Score: 0.8804\n",
      "=======================================\n",
      "Wall time: 3.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Stemmer / Unigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1), min_df=10)\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6964\n",
      "F1 Score: 0.6964\n",
      "=======================================\n",
      "Wall time: 50 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8805\n",
      "F1 Score: 0.8805\n",
      "=======================================\n",
      "Wall time: 3.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8302\n",
      "F1 Score: 0.8302\n",
      "=======================================\n",
      "Wall time: 31.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8443\n",
      "F1 Score: 0.8443\n",
      "=======================================\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8551\n",
      "F1 Score: 0.8551\n",
      "=======================================\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Stemmer / Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2), min_df=10)\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6862\n",
      "F1 Score: 0.6862\n",
      "=======================================\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8623\n",
      "F1 Score: 0.8623\n",
      "=======================================\n",
      "Wall time: 5.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8214\n",
      "F1 Score: 0.8213\n",
      "=======================================\n",
      "Wall time: 35.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8727\n",
      "F1 Score: 0.8727\n",
      "=======================================\n",
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8734\n",
      "F1 Score: 0.8733\n",
      "=======================================\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Stemmer / Unigrama - Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), min_df=10)\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6994\n",
      "F1 Score: 0.6994\n",
      "=======================================\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8891\n",
      "F1 Score: 0.8891\n",
      "=======================================\n",
      "Wall time: 8.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.841\n",
      "F1 Score: 0.841\n",
      "=======================================\n",
      "Wall time: 43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8713\n",
      "F1 Score: 0.8713\n",
      "=======================================\n",
      "Wall time: 2.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8731\n",
      "F1 Score: 0.8731\n",
      "=======================================\n",
      "Wall time: 2.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Stemmer / Unigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1), min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6912\n",
      "F1 Score: 0.6912\n",
      "=======================================\n",
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8767\n",
      "F1 Score: 0.8767\n",
      "=======================================\n",
      "Wall time: 3.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8325\n",
      "F1 Score: 0.8325\n",
      "=======================================\n",
      "Wall time: 30.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8434\n",
      "F1 Score: 0.8433\n",
      "=======================================\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8536\n",
      "F1 Score: 0.8536\n",
      "=======================================\n",
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Stemmer / Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2), min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6904\n",
      "F1 Score: 0.6904\n",
      "=======================================\n",
      "Wall time: 50.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8565\n",
      "F1 Score: 0.8564\n",
      "=======================================\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8059\n",
      "F1 Score: 0.8057\n",
      "=======================================\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8695\n",
      "F1 Score: 0.8695\n",
      "=======================================\n",
      "Wall time: 1.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8677\n",
      "F1 Score: 0.8676\n",
      "=======================================\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Stemmer / Unigrama - Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6965\n",
      "F1 Score: 0.6965\n",
      "=======================================\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8866\n",
      "F1 Score: 0.8866\n",
      "=======================================\n",
      "Wall time: 6.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8411\n",
      "F1 Score: 0.8411\n",
      "=======================================\n",
      "Wall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8746\n",
      "F1 Score: 0.8746\n",
      "=======================================\n",
      "Wall time: 2.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8748\n",
      "F1 Score: 0.8747\n",
      "=======================================\n",
      "Wall time: 2.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.712\n",
      "F1 Score: 0.712\n",
      "=======================================\n",
      "Wall time: 55.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8823\n",
      "F1 Score: 0.8823\n",
      "=======================================\n",
      "Wall time: 5.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8487\n",
      "F1 Score: 0.8487\n",
      "=======================================\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8517\n",
      "F1 Score: 0.8516\n",
      "=======================================\n",
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8552\n",
      "F1 Score: 0.8551\n",
      "=======================================\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6938\n",
      "F1 Score: 0.6938\n",
      "=======================================\n",
      "Wall time: 4min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8826\n",
      "F1 Score: 0.8826\n",
      "=======================================\n",
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8266\n",
      "F1 Score: 0.8265\n",
      "=======================================\n",
      "Wall time: 7min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8836\n",
      "F1 Score: 0.8835\n",
      "=======================================\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8822\n",
      "F1 Score: 0.882\n",
      "=======================================\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma / Unigrama - Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7195\n",
      "F1 Score: 0.7195\n",
      "=======================================\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.9007\n",
      "F1 Score: 0.9007\n",
      "=======================================\n",
      "Wall time: 47.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8516\n",
      "F1 Score: 0.8516\n",
      "=======================================\n",
      "Wall time: 5min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8811\n",
      "F1 Score: 0.8809\n",
      "=======================================\n",
      "Wall time: 3.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8819\n",
      "F1 Score: 0.8818\n",
      "=======================================\n",
      "Wall time: 3.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma / Unigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1), min_df=10)\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7113\n",
      "F1 Score: 0.7113\n",
      "=======================================\n",
      "Wall time: 48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8754\n",
      "F1 Score: 0.8754\n",
      "=======================================\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8401\n",
      "F1 Score: 0.84\n",
      "=======================================\n",
      "Wall time: 30.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8463\n",
      "F1 Score: 0.8463\n",
      "=======================================\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8524\n",
      "F1 Score: 0.8524\n",
      "=======================================\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma / Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2), min_df=10)\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6857\n",
      "F1 Score: 0.6857\n",
      "=======================================\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8782\n",
      "F1 Score: 0.8782\n",
      "=======================================\n",
      "Wall time: 5.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8308\n",
      "F1 Score: 0.8308\n",
      "=======================================\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8767\n",
      "F1 Score: 0.8767\n",
      "=======================================\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8722\n",
      "F1 Score: 0.8721\n",
      "=======================================\n",
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma / Unigrama - Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), min_df=10)\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7213\n",
      "F1 Score: 0.7213\n",
      "=======================================\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8921\n",
      "F1 Score: 0.8921\n",
      "=======================================\n",
      "Wall time: 8.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8518\n",
      "F1 Score: 0.8518\n",
      "=======================================\n",
      "Wall time: 42.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8699\n",
      "F1 Score: 0.8699\n",
      "=======================================\n",
      "Wall time: 2.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8759\n",
      "F1 Score: 0.8759\n",
      "=======================================\n",
      "Wall time: 2.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma / Unigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1), min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7116\n",
      "F1 Score: 0.7116\n",
      "=======================================\n",
      "Wall time: 46.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8768\n",
      "F1 Score: 0.8768\n",
      "=======================================\n",
      "Wall time: 3.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8414\n",
      "F1 Score: 0.8414\n",
      "=======================================\n",
      "Wall time: 30 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8479\n",
      "F1 Score: 0.8478\n",
      "=======================================\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.852\n",
      "F1 Score: 0.8519\n",
      "=======================================\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma / Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2), min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7208\n",
      "F1 Score: 0.7207\n",
      "=======================================\n",
      "Wall time: 56.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8702\n",
      "F1 Score: 0.8701\n",
      "=======================================\n",
      "Wall time: 4.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8287\n",
      "F1 Score: 0.8287\n",
      "=======================================\n",
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8772\n",
      "F1 Score: 0.8772\n",
      "=======================================\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8736\n",
      "F1 Score: 0.8736\n",
      "=======================================\n",
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma / Unigrama - Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7199\n",
      "F1 Score: 0.7199\n",
      "=======================================\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8888\n",
      "F1 Score: 0.8888\n",
      "=======================================\n",
      "Wall time: 6.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8511\n",
      "F1 Score: 0.8511\n",
      "=======================================\n",
      "Wall time: 38.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.872\n",
      "F1 Score: 0.872\n",
      "=======================================\n",
      "Wall time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.875\n",
      "F1 Score: 0.875\n",
      "=======================================\n",
      "Wall time: 2.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma_Verb Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7071\n",
      "F1 Score: 0.7071\n",
      "=======================================\n",
      "Wall time: 59.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8797\n",
      "F1 Score: 0.8797\n",
      "=======================================\n",
      "Wall time: 5.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8449\n",
      "F1 Score: 0.8449\n",
      "=======================================\n",
      "Wall time: 37.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8542\n",
      "F1 Score: 0.8541\n",
      "=======================================\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8586\n",
      "F1 Score: 0.8585\n",
      "=======================================\n",
      "Wall time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma_Verb Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.691\n",
      "F1 Score: 0.691\n",
      "=======================================\n",
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8813\n",
      "F1 Score: 0.8813\n",
      "=======================================\n",
      "Wall time: 42.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8294\n",
      "F1 Score: 0.8294\n",
      "=======================================\n",
      "Wall time: 8min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8831\n",
      "F1 Score: 0.8829\n",
      "=======================================\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8826\n",
      "F1 Score: 0.8825\n",
      "=======================================\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma_Verb / Unigrama - Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7167\n",
      "F1 Score: 0.7167\n",
      "=======================================\n",
      "Wall time: 5min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8996\n",
      "F1 Score: 0.8996\n",
      "=======================================\n",
      "Wall time: 51.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8531\n",
      "F1 Score: 0.8531\n",
      "=======================================\n",
      "Wall time: 5min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8812\n",
      "F1 Score: 0.881\n",
      "=======================================\n",
      "Wall time: 4.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8823\n",
      "F1 Score: 0.8822\n",
      "=======================================\n",
      "Wall time: 3.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma_Verb / Unigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1), min_df=10)\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7085\n",
      "F1 Score: 0.7085\n",
      "=======================================\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8761\n",
      "F1 Score: 0.8761\n",
      "=======================================\n",
      "Wall time: 5.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.842\n",
      "F1 Score: 0.842\n",
      "=======================================\n",
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8477\n",
      "F1 Score: 0.8476\n",
      "=======================================\n",
      "Wall time: 3.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8549\n",
      "F1 Score: 0.8549\n",
      "=======================================\n",
      "Wall time: 3.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma_Verb / Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2), min_df=10)\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6893\n",
      "F1 Score: 0.6893\n",
      "=======================================\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.874\n",
      "F1 Score: 0.874\n",
      "=======================================\n",
      "Wall time: 5.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.819\n",
      "F1 Score: 0.8188\n",
      "=======================================\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8779\n",
      "F1 Score: 0.8779\n",
      "=======================================\n",
      "Wall time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8725\n",
      "F1 Score: 0.8724\n",
      "=======================================\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma_Verb / Unigrama - Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), min_df=10)\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.714\n",
      "F1 Score: 0.714\n",
      "=======================================\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8896\n",
      "F1 Score: 0.8896\n",
      "=======================================\n",
      "Wall time: 8.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8488\n",
      "F1 Score: 0.8488\n",
      "=======================================\n",
      "Wall time: 43.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.875\n",
      "F1 Score: 0.875\n",
      "=======================================\n",
      "Wall time: 2.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8751\n",
      "F1 Score: 0.8751\n",
      "=======================================\n",
      "Wall time: 2.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma_Verb / Unigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1), min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7108\n",
      "F1 Score: 0.7108\n",
      "=======================================\n",
      "Wall time: 49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8776\n",
      "F1 Score: 0.8776\n",
      "=======================================\n",
      "Wall time: 3.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8456\n",
      "F1 Score: 0.8455\n",
      "=======================================\n",
      "Wall time: 30.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8481\n",
      "F1 Score: 0.8481\n",
      "=======================================\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8549\n",
      "F1 Score: 0.8548\n",
      "=======================================\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma_Verb / Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2), min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7138\n",
      "F1 Score: 0.7138\n",
      "=======================================\n",
      "Wall time: 53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8713\n",
      "F1 Score: 0.8713\n",
      "=======================================\n",
      "Wall time: 4.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8203\n",
      "F1 Score: 0.8202\n",
      "=======================================\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.877\n",
      "F1 Score: 0.877\n",
      "=======================================\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8711\n",
      "F1 Score: 0.8709\n",
      "=======================================\n",
      "Wall time: 1.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Lemma_Verb / Unigrama - Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7132\n",
      "F1 Score: 0.7132\n",
      "=======================================\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8893\n",
      "F1 Score: 0.8892\n",
      "=======================================\n",
      "Wall time: 7.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8545\n",
      "F1 Score: 0.8545\n",
      "=======================================\n",
      "Wall time: 38.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8732\n",
      "F1 Score: 0.8732\n",
      "=======================================\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.876\n",
      "F1 Score: 0.876\n",
      "=======================================\n",
      "Wall time: 2.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=======================================\")\n",
    "print(\"=======================================\")\n",
    "print(\"=======================================\")\n",
    "print(\"=======================================\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Texto Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1')\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7062\n",
      "F1 Score: 0.7061\n",
      "=======================================\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8171\n",
      "F1 Score: 0.8169\n",
      "=======================================\n",
      "Wall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8298\n",
      "F1 Score: 0.8295\n",
      "=======================================\n",
      "Wall time: 7min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8568\n",
      "F1 Score: 0.8562\n",
      "=======================================\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8606\n",
      "F1 Score: 0.8606\n",
      "=======================================\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Texto Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1')\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6728\n",
      "F1 Score: 0.6728\n",
      "=======================================\n",
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.823\n",
      "F1 Score: 0.8226\n",
      "=======================================\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8298\n",
      "F1 Score: 0.8295\n",
      "=======================================\n",
      "Wall time: 7min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8343\n",
      "F1 Score: 0.832\n",
      "=======================================\n",
      "Wall time: 2.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8837\n",
      "F1 Score: 0.8836\n",
      "=======================================\n",
      "Wall time: 2.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Texto Unigrama - Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1')\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7\n",
      "F1 Score: 0.6998\n",
      "=======================================\n",
      "Wall time: 6min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8119\n",
      "F1 Score: 0.8116\n",
      "=======================================\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8314\n",
      "F1 Score: 0.8314\n",
      "=======================================\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8405\n",
      "F1 Score: 0.8385\n",
      "=======================================\n",
      "Wall time: 4.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8814\n",
      "F1 Score: 0.8813\n",
      "=======================================\n",
      "Wall time: 4.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Texto Unigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1', min_df=10)\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7038\n",
      "F1 Score: 0.7038\n",
      "=======================================\n",
      "Wall time: 59.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8235\n",
      "F1 Score: 0.8233\n",
      "=======================================\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8314\n",
      "F1 Score: 0.8314\n",
      "=======================================\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8574\n",
      "F1 Score: 0.857\n",
      "=======================================\n",
      "Wall time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8563\n",
      "F1 Score: 0.8563\n",
      "=======================================\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Texto Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1', min_df=10)\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.681\n",
      "F1 Score: 0.681\n",
      "=======================================\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8297\n",
      "F1 Score: 0.8297\n",
      "=======================================\n",
      "Wall time: 2.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8196\n",
      "F1 Score: 0.8194\n",
      "=======================================\n",
      "Wall time: 31.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8688\n",
      "F1 Score: 0.8687\n",
      "=======================================\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8726\n",
      "F1 Score: 0.8725\n",
      "=======================================\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Texto Unigrama - Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1', min_df=10)\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7064\n",
      "F1 Score: 0.7064\n",
      "=======================================\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8244\n",
      "F1 Score: 0.8244\n",
      "=======================================\n",
      "Wall time: 4.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8474\n",
      "F1 Score: 0.8473\n",
      "=======================================\n",
      "Wall time: 39.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8685\n",
      "F1 Score: 0.8682\n",
      "=======================================\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.877\n",
      "F1 Score: 0.877\n",
      "=======================================\n",
      "Wall time: 3.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Texto Unigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7125\n",
      "F1 Score: 0.7124\n",
      "=======================================\n",
      "Wall time: 55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8284\n",
      "F1 Score: 0.8281\n",
      "=======================================\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8397\n",
      "F1 Score: 0.8397\n",
      "=======================================\n",
      "Wall time: 28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8577\n",
      "F1 Score: 0.8574\n",
      "=======================================\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8541\n",
      "F1 Score: 0.8541\n",
      "=======================================\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Texto Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7041\n",
      "F1 Score: 0.7041\n",
      "=======================================\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8319\n",
      "F1 Score: 0.8318\n",
      "=======================================\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8259\n",
      "F1 Score: 0.8259\n",
      "=======================================\n",
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8696\n",
      "F1 Score: 0.8696\n",
      "=======================================\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8732\n",
      "F1 Score: 0.8731\n",
      "=======================================\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Texto Unigrama - Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7169\n",
      "F1 Score: 0.7169\n",
      "=======================================\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.838\n",
      "F1 Score: 0.8379\n",
      "=======================================\n",
      "Wall time: 3.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8483\n",
      "F1 Score: 0.8482\n",
      "=======================================\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8706\n",
      "F1 Score: 0.8705\n",
      "=======================================\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8755\n",
      "F1 Score: 0.8755\n",
      "=======================================\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Texto Unigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras / Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['amos', 'atr', 'atrav', 'dever', 'ent', 'est', 'estiv', 'far', 'houv', 'ltimo', 'mero', 'meros', 'nhamos', 'porqu', 'posi', 'poss', 'prio', 'quest', 'ramos', 'rea', 'rela', 'rios', 'ssemos', 'tamb', 'tima', 'timo', 'tiv', 'vamos', 'vel', 'voc', 'xima', 'ximo'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}', stop_words=stops)\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7041\n",
      "F1 Score: 0.7041\n",
      "=======================================\n",
      "Wall time: 44.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8362\n",
      "F1 Score: 0.8358\n",
      "=======================================\n",
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8423\n",
      "F1 Score: 0.8423\n",
      "=======================================\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8615\n",
      "F1 Score: 0.8614\n",
      "=======================================\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8489\n",
      "F1 Score: 0.8489\n",
      "=======================================\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Texto Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras / Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['amos', 'atr', 'atrav', 'dever', 'ent', 'est', 'estiv', 'far', 'houv', 'ltimo', 'mero', 'meros', 'nhamos', 'porqu', 'posi', 'poss', 'prio', 'quest', 'ramos', 'rea', 'rela', 'rios', 'ssemos', 'tamb', 'tima', 'timo', 'tiv', 'vamos', 'vel', 'voc', 'xima', 'ximo'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}', stop_words=stops)\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7171\n",
      "F1 Score: 0.7171\n",
      "=======================================\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8103\n",
      "F1 Score: 0.8097\n",
      "=======================================\n",
      "Wall time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.7942\n",
      "F1 Score: 0.794\n",
      "=======================================\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8454\n",
      "F1 Score: 0.8454\n",
      "=======================================\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8466\n",
      "F1 Score: 0.8463\n",
      "=======================================\n",
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Texto Unigrama - Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras / Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FIAP\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['amos', 'atr', 'atrav', 'dever', 'ent', 'est', 'estiv', 'far', 'houv', 'ltimo', 'mero', 'meros', 'nhamos', 'porqu', 'posi', 'poss', 'prio', 'quest', 'ramos', 'rea', 'rela', 'rios', 'ssemos', 'tamb', 'tima', 'timo', 'tiv', 'vamos', 'vel', 'voc', 'xima', 'ximo'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}', stop_words=stops)\n",
    "vect.fit(df_treino.texto)\n",
    "text_vect_treino = vect.transform(df_treino.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7155\n",
      "F1 Score: 0.7155\n",
      "=======================================\n",
      "Wall time: 58.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8394\n",
      "F1 Score: 0.839\n",
      "=======================================\n",
      "Wall time: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8483\n",
      "F1 Score: 0.8482\n",
      "=======================================\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8708\n",
      "F1 Score: 0.8708\n",
      "=======================================\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.869\n",
      "F1 Score: 0.869\n",
      "=======================================\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.texto)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Stemmer Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1')\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6902\n",
      "F1 Score: 0.6902\n",
      "=======================================\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8139\n",
      "F1 Score: 0.8136\n",
      "=======================================\n",
      "Wall time: 2.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8275\n",
      "F1 Score: 0.8274\n",
      "=======================================\n",
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8452\n",
      "F1 Score: 0.8443\n",
      "=======================================\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8584\n",
      "F1 Score: 0.8583\n",
      "=======================================\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Stemmer Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1')\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6639\n",
      "F1 Score: 0.6639\n",
      "=======================================\n",
      "Wall time: 5min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8182\n",
      "F1 Score: 0.8178\n",
      "=======================================\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8161\n",
      "F1 Score: 0.8157\n",
      "=======================================\n",
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8284\n",
      "F1 Score: 0.8255\n",
      "=======================================\n",
      "Wall time: 2.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8812\n",
      "F1 Score: 0.881\n",
      "=======================================\n",
      "Wall time: 2.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Stemmer / Unigrama - Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1')\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6826\n",
      "F1 Score: 0.6825\n",
      "=======================================\n",
      "Wall time: 5min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8061\n",
      "F1 Score: 0.8059\n",
      "=======================================\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8403\n",
      "F1 Score: 0.8402\n",
      "=======================================\n",
      "Wall time: 4min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8295\n",
      "F1 Score: 0.8267\n",
      "=======================================\n",
      "Wall time: 3.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8805\n",
      "F1 Score: 0.8804\n",
      "=======================================\n",
      "Wall time: 3.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Stemmer / Unigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1', min_df=10)\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6909\n",
      "F1 Score: 0.6909\n",
      "=======================================\n",
      "Wall time: 54.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.818\n",
      "F1 Score: 0.8178\n",
      "=======================================\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8301\n",
      "F1 Score: 0.83\n",
      "=======================================\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8517\n",
      "F1 Score: 0.8513\n",
      "=======================================\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8551\n",
      "F1 Score: 0.8551\n",
      "=======================================\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Stemmer / Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1', min_df=10)\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6763\n",
      "F1 Score: 0.6763\n",
      "=======================================\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8276\n",
      "F1 Score: 0.8276\n",
      "=======================================\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8165\n",
      "F1 Score: 0.8163\n",
      "=======================================\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8627\n",
      "F1 Score: 0.8626\n",
      "=======================================\n",
      "Wall time: 2.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8734\n",
      "F1 Score: 0.8733\n",
      "=======================================\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Stemmer / Unigrama - Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1', min_df=10)\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6885\n",
      "F1 Score: 0.6885\n",
      "=======================================\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8189\n",
      "F1 Score: 0.8189\n",
      "=======================================\n",
      "Wall time: 4.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8436\n",
      "F1 Score: 0.8436\n",
      "=======================================\n",
      "Wall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8643\n",
      "F1 Score: 0.864\n",
      "=======================================\n",
      "Wall time: 2.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8731\n",
      "F1 Score: 0.8731\n",
      "=======================================\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Stemmer / Unigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6861\n",
      "F1 Score: 0.686\n",
      "=======================================\n",
      "Wall time: 49.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8222\n",
      "F1 Score: 0.822\n",
      "=======================================\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8271\n",
      "F1 Score: 0.8271\n",
      "=======================================\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8493\n",
      "F1 Score: 0.8488\n",
      "=======================================\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8536\n",
      "F1 Score: 0.8536\n",
      "=======================================\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Stemmer / Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6871\n",
      "F1 Score: 0.6871\n",
      "=======================================\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8208\n",
      "F1 Score: 0.8208\n",
      "=======================================\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8037\n",
      "F1 Score: 0.8035\n",
      "=======================================\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8588\n",
      "F1 Score: 0.8587\n",
      "=======================================\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8677\n",
      "F1 Score: 0.8676\n",
      "=======================================\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Stemmer / Unigrama - Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.stemmer)\n",
    "text_vect_treino = vect.transform(df_treino.stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6868\n",
      "F1 Score: 0.6868\n",
      "=======================================\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8296\n",
      "F1 Score: 0.8296\n",
      "=======================================\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8371\n",
      "F1 Score: 0.8371\n",
      "=======================================\n",
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8667\n",
      "F1 Score: 0.8665\n",
      "=======================================\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8748\n",
      "F1 Score: 0.8747\n",
      "=======================================\n",
      "Wall time: 2.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.stemmer)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1')\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7096\n",
      "F1 Score: 0.7096\n",
      "=======================================\n",
      "Wall time: 56.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8203\n",
      "F1 Score: 0.8201\n",
      "=======================================\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8389\n",
      "F1 Score: 0.8389\n",
      "=======================================\n",
      "Wall time: 30.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8506\n",
      "F1 Score: 0.8498\n",
      "=======================================\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8552\n",
      "F1 Score: 0.8551\n",
      "=======================================\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1')\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6742\n",
      "F1 Score: 0.674\n",
      "=======================================\n",
      "Wall time: 4min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8199\n",
      "F1 Score: 0.8197\n",
      "=======================================\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8262\n",
      "F1 Score: 0.8258\n",
      "=======================================\n",
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8233\n",
      "F1 Score: 0.8197\n",
      "=======================================\n",
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8822\n",
      "F1 Score: 0.882\n",
      "=======================================\n",
      "Wall time: 2.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma / Unigrama - Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1')\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7085\n",
      "F1 Score: 0.7085\n",
      "=======================================\n",
      "Wall time: 4min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8115\n",
      "F1 Score: 0.8113\n",
      "=======================================\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8415\n",
      "F1 Score: 0.8413\n",
      "=======================================\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8298\n",
      "F1 Score: 0.827\n",
      "=======================================\n",
      "Wall time: 3.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8819\n",
      "F1 Score: 0.8818\n",
      "=======================================\n",
      "Wall time: 3.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma / Unigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1', min_df=10)\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7044\n",
      "F1 Score: 0.7044\n",
      "=======================================\n",
      "Wall time: 51.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.825\n",
      "F1 Score: 0.8248\n",
      "=======================================\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8375\n",
      "F1 Score: 0.8375\n",
      "=======================================\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8539\n",
      "F1 Score: 0.8534\n",
      "=======================================\n",
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8524\n",
      "F1 Score: 0.8524\n",
      "=======================================\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma / Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1', min_df=10)\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6775\n",
      "F1 Score: 0.6775\n",
      "=======================================\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.829\n",
      "F1 Score: 0.829\n",
      "=======================================\n",
      "Wall time: 3.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8239\n",
      "F1 Score: 0.8237\n",
      "=======================================\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8688\n",
      "F1 Score: 0.8686\n",
      "=======================================\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8722\n",
      "F1 Score: 0.8721\n",
      "=======================================\n",
      "Wall time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TF-IDF Vectorizer Lemma / Unigrama - Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1', min_df=10)\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7061\n",
      "F1 Score: 0.7061\n",
      "=======================================\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8284\n",
      "F1 Score: 0.8283\n",
      "=======================================\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8549\n",
      "F1 Score: 0.8549\n",
      "=======================================\n",
      "Wall time: 38.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8661\n",
      "F1 Score: 0.8658\n",
      "=======================================\n",
      "Wall time: 2.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8759\n",
      "F1 Score: 0.8759\n",
      "=======================================\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma / Unigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7085\n",
      "F1 Score: 0.7085\n",
      "=======================================\n",
      "Wall time: 51.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8289\n",
      "F1 Score: 0.8287\n",
      "=======================================\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8389\n",
      "F1 Score: 0.8389\n",
      "=======================================\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8542\n",
      "F1 Score: 0.8538\n",
      "=======================================\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.852\n",
      "F1 Score: 0.8519\n",
      "=======================================\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma / Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6995\n",
      "F1 Score: 0.6995\n",
      "=======================================\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8304\n",
      "F1 Score: 0.8303\n",
      "=======================================\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8317\n",
      "F1 Score: 0.8317\n",
      "=======================================\n",
      "Wall time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8702\n",
      "F1 Score: 0.8701\n",
      "=======================================\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8736\n",
      "F1 Score: 0.8736\n",
      "=======================================\n",
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma / Unigrama - Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.lemma)\n",
    "text_vect_treino = vect.transform(df_treino.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7149\n",
      "F1 Score: 0.7149\n",
      "=======================================\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8372\n",
      "F1 Score: 0.8372\n",
      "=======================================\n",
      "Wall time: 4.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8524\n",
      "F1 Score: 0.8524\n",
      "=======================================\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8694\n",
      "F1 Score: 0.8692\n",
      "=======================================\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.875\n",
      "F1 Score: 0.875\n",
      "=======================================\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma_Verb Unigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1')\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6945\n",
      "F1 Score: 0.6945\n",
      "=======================================\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8193\n",
      "F1 Score: 0.819\n",
      "=======================================\n",
      "Wall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8368\n",
      "F1 Score: 0.8368\n",
      "=======================================\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8495\n",
      "F1 Score: 0.8486\n",
      "=======================================\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8586\n",
      "F1 Score: 0.8585\n",
      "=======================================\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma_Verb Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1')\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6683\n",
      "F1 Score: 0.6682\n",
      "=======================================\n",
      "Wall time: 5min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8171\n",
      "F1 Score: 0.8168\n",
      "=======================================\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8243\n",
      "F1 Score: 0.8239\n",
      "=======================================\n",
      "Wall time: 6min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8224\n",
      "F1 Score: 0.819\n",
      "=======================================\n",
      "Wall time: 2.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8826\n",
      "F1 Score: 0.8825\n",
      "=======================================\n",
      "Wall time: 2.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TF-IDF Vectorizer Lemma_Verb / Unigrama - Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1')\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6976\n",
      "F1 Score: 0.6976\n",
      "=======================================\n",
      "Wall time: 5min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8086\n",
      "F1 Score: 0.8084\n",
      "=======================================\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8434\n",
      "F1 Score: 0.8433\n",
      "=======================================\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8289\n",
      "F1 Score: 0.8261\n",
      "=======================================\n",
      "Wall time: 3.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8823\n",
      "F1 Score: 0.8822\n",
      "=======================================\n",
      "Wall time: 3.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma_Verb / Unigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1', min_df=10)\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7038\n",
      "F1 Score: 0.7038\n",
      "=======================================\n",
      "Wall time: 57.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8249\n",
      "F1 Score: 0.8246\n",
      "=======================================\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8458\n",
      "F1 Score: 0.8458\n",
      "=======================================\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8526\n",
      "F1 Score: 0.8521\n",
      "=======================================\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8549\n",
      "F1 Score: 0.8549\n",
      "=======================================\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma_Verb / Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1', min_df=10)\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6735\n",
      "F1 Score: 0.6735\n",
      "=======================================\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8317\n",
      "F1 Score: 0.8317\n",
      "=======================================\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8217\n",
      "F1 Score: 0.8215\n",
      "=======================================\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8678\n",
      "F1 Score: 0.8677\n",
      "=======================================\n",
      "Wall time: 2.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8725\n",
      "F1 Score: 0.8724\n",
      "=======================================\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma_Verb / Unigrama - Bigrama / MindF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1', min_df=10)\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7103\n",
      "F1 Score: 0.7103\n",
      "=======================================\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8257\n",
      "F1 Score: 0.8256\n",
      "=======================================\n",
      "Wall time: 4.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8524\n",
      "F1 Score: 0.8524\n",
      "=======================================\n",
      "Wall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8658\n",
      "F1 Score: 0.8654\n",
      "=======================================\n",
      "Wall time: 2.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8751\n",
      "F1 Score: 0.8751\n",
      "=======================================\n",
      "Wall time: 3.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma_Verb / Unigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,1), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.7028\n",
      "F1 Score: 0.7028\n",
      "=======================================\n",
      "Wall time: 51.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8288\n",
      "F1 Score: 0.8285\n",
      "=======================================\n",
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8388\n",
      "F1 Score: 0.8388\n",
      "=======================================\n",
      "Wall time: 27.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8556\n",
      "F1 Score: 0.8551\n",
      "=======================================\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8549\n",
      "F1 Score: 0.8548\n",
      "=======================================\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma_Verb / Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,2), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.6979\n",
      "F1 Score: 0.6979\n",
      "=======================================\n",
      "Wall time: 58.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8292\n",
      "F1 Score: 0.8291\n",
      "=======================================\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.826\n",
      "F1 Score: 0.826\n",
      "=======================================\n",
      "Wall time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8698\n",
      "F1 Score: 0.8698\n",
      "=======================================\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.8711\n",
      "F1 Score: 0.8709\n",
      "=======================================\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer Lemma_Verb / Unigrama - Bigrama / MindF = 10 / token_pattern = Palavras com mais de 3 letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2), smooth_idf=False, norm='l1', min_df=10, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "vect.fit(df_treino.lemma_verb)\n",
    "text_vect_treino = vect.transform(df_treino.lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Acuracidade: 0.708\n",
      "F1 Score: 0.708\n",
      "=======================================\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Acuracidade: 0.8384\n",
      "F1 Score: 0.8383\n",
      "=======================================\n",
      "Wall time: 3.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Acuracidade: 0.8488\n",
      "F1 Score: 0.8488\n",
      "=======================================\n",
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Random Forest\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB\n",
      "Acuracidade: 0.8709\n",
      "F1 Score: 0.8708\n",
      "=======================================\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Multinomial NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "Acuracidade: 0.876\n",
      "F1 Score: 0.876\n",
      "=======================================\n",
      "Wall time: 2.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(text_vect_treino, df_treino.sentimento)\n",
    "\n",
    "# vetorização do dataframe de teste\n",
    "text_vect_teste = vect.transform(df_teste.lemma_verb)\n",
    "\n",
    "# escoragem da classificação na amostra de teste (textos vetorizados)\n",
    "y_predicao = model.predict(text_vect_teste)\n",
    "\n",
    "# mensuração do resultado pela acurácia\n",
    "y_teste = df_teste.sentimento\n",
    "\n",
    "accuracy = accuracy_score(df_teste.sentimento, y_predicao)\n",
    "f1score = f1_score(df_teste.sentimento, y_predicao, average='weighted')\n",
    "print(\"Bernoulli NB\")\n",
    "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
    "print(f\"F1 Score: {round(f1score,4)}\")\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68SiMjcWqD_m"
   },
   "source": [
    "#### **Validação do professor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T24EasckqG2I"
   },
   "source": [
    "Consolidar apenas os scripts do seu **modelo campeão**, desde o carregamento do dataframe, separação das amostras, tratamentos utilizados (funções, limpezas, etc.), criação dos objetos de vetorização dos textos e modelo treinado e outras implementações utilizadas no processo de desenvolvimento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "id": "ZxqHA-XCrqsD",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import plotly.express as px\n",
    "\n",
    "#fig = px.line(x=[\"a\",\"b\",\"c\"], y=[1,3,2], title=\"sample figure\")\n",
    "#print(fig)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuJtvcfXo3J4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ULYNH6-o3Hf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ClM-JTJo3FK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TemplateTrabalhoFinal-NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
