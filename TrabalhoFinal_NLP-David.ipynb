{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDbi6PDS9MYO"
   },
   "source": [
    "***Participantes (RM - NOME):***<br>\n",
    "RM339661 - David Claro<br>\n",
    "xxxx - xxxxx<br>\n",
    "xxxx - xxxxx<br>\n",
    "xxxx - xxxxx<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xw6WhaNo4k3"
   },
   "source": [
    "###**Criar um classificador de sentimento aplicando técnicas de PLN**\n",
    "---\n",
    "\n",
    "Utilizando o dataset de revisões de filmes em português [1], criar um classificador de sentimentos que consiga um score na métrica F1 Score superior a 70%.\n",
    "\n",
    "Devem utilizar uma amostra de 20% e randon_state igual a 42 para testar as implementações e mensurar a métrica F1 Score (usar o parâmetro average = 'weighted') o restante dos dados devem ser utilizados para o treinamento (80%).\n",
    "\n",
    "Fique a vontade para testar os métodos de pré-processamento, abordagens, algoritmos e bibliotecas, mas explique e justifique suas decisões.\n",
    "O trabalho poderá ser feito em grupo de até 4 pessoas (mesmo grupo do Startup One).\n",
    "\n",
    "Separe a implementação do seu modelo campeão junto com a parte de validação/teste de forma que o professor consiga executar todo o pipeline do modelo campeão.\n",
    "\n",
    "Composição da nota:\n",
    "- 50% - Demonstrações das aplicações das técnicas de PLN (regras, pré-processamentos, tratamentos, variedade de modelos aplicados, etc.)\n",
    "- 50% - Baseado na performance obtida com o dataset de teste (conforme recomendação da amostra) no seu modelo campeão e na validação que o professor processar (Métrica F1 Score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzhQpodBpRpX"
   },
   "source": [
    "[1] - https://dados-ml-pln.s3-sa-east-1.amazonaws.com/reviews-pt-br.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5550,
     "status": "ok",
     "timestamp": 1623118151010,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "DMBI8SQtps1n"
   },
   "outputs": [],
   "source": [
    "# CARREGANDO O DATA FRAME\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://dados-ml-pln.s3-sa-east-1.amazonaws.com/reviews-pt-br.csv', encoding='utf-8')\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "# Façam o download do arquivo e utilizem localmente durante os testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1623118151011,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "s__lBzDQwrcG",
    "outputId": "7aafdd62-98c0-4a54-e6fd-158b2e6e7e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44514 entries, 0 to 44513\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   codigo      44514 non-null  int64 \n",
      " 1   texto       44514 non-null  object\n",
      " 2   sentimento  44514 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyKC9Vhkp0BK"
   },
   "source": [
    "Bom desenvolvimento!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13803,
     "status": "ok",
     "timestamp": 1623118164790,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "nze8UbKhosm9",
    "outputId": "6d7c85e9-895b-41b7-ca94-f63b60b0800e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
      "Collecting pt_core_news_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2MB)\n",
      "\u001b[K     |████████████████████████████████| 21.2MB 1.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (57.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.7.4.3)\n",
      "Building wheels for collected packages: pt-core-news-sm\n",
      "  Building wheel for pt-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pt-core-news-sm: filename=pt_core_news_sm-2.2.5-cp37-none-any.whl size=21186282 sha256=43a20dbfbd53e41a58b22c56b39c070a8c74ec25aed83a5959e7597d2b978680\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tb4k0xf5/wheels/ea/94/74/ec9be8418e9231b471be5dc7e1b45dd670019a376a6b5bc1c0\n",
      "Successfully built pt-core-news-sm\n",
      "Installing collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('pt_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/pt_core_news_sm -->\n",
      "/usr/local/lib/python3.7/dist-packages/spacy/data/pt\n",
      "You can now load the model via spacy.load('pt')\n"
     ]
    }
   ],
   "source": [
    "#!pip install scipy==1.2.0\n",
    "#!pip install gensim==2.0.0\n",
    "!pip install spacy\n",
    "!python -m spacy download pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1789,
     "status": "ok",
     "timestamp": 1623118166565,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "ue0nV0uVo3OZ"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3597,
     "status": "ok",
     "timestamp": 1623118170146,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "FziwgqJmw9OD",
    "outputId": "07e3ccf1-a927-4a2b-9525-9151ab807b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega StopWords\n",
    "nlp = spacy.load('pt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# stopwords NLTK\n",
    "stops_nltk = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# stopwords SpaCy\n",
    "stops_spacy = nlp.Defaults.stop_words\n",
    "\n",
    "# stopwords do SpaCy e NLTK combinadas\n",
    "stops = list(set(stops_spacy).union(set(stops_nltk)))\n",
    "len(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1623118170146,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "25cBRwGAw8-1"
   },
   "outputs": [],
   "source": [
    "# função de lematização completa do documento\n",
    "\n",
    "\n",
    "#def fn_lematiza_texto(texto):\n",
    "#    sent = []\n",
    "#    doc = nlp(texto)\n",
    "#    for word in doc:\n",
    "#        sent.append(word.lemma_)\n",
    "#\n",
    "#    return ' '.join(sent)\n",
    "\n",
    "# função de lematização dos verbos do documento\n",
    "#def fn_lematiza_verb_texto(texto):\n",
    "#    sent = []\n",
    "#    doc = nlp(texto)\n",
    "#    for word in doc:\n",
    "#        if word.pos_ =='VERB':\n",
    "#            sent.append(word.lemma_)\n",
    "#        else:\n",
    "#            sent.append(word.text)\n",
    "#    return \" \".join(sent)\n",
    "\n",
    "\n",
    "# função para limpar documento\n",
    "def fn_limpa_texto(texto):\n",
    "    texto = re.sub(r'@\\w+','',texto)\n",
    "    texto = re.sub(r'#','',texto)\n",
    "    texto = re.sub(r'RT[\\s]+','',texto)  \n",
    "    #texto = re.sub(r'rt[\\s]+','',texto)  \n",
    "    texto = re.sub(r'https:/\\/\\S+','',texto)\n",
    "    texto = re.sub(r'[,.:;]','',texto)\n",
    "    texto = re.sub(r'\\n',' ',texto)\n",
    "    texto = re.sub(u'[^a-zA-ZâêîôûÂÊÎÔÛáéíóúÁÉÍÓÚàèìòùÀÈÌÒÙãõÃÕçÇ ]', '', texto)\n",
    "    texto = texto.strip().lower()\n",
    "    \n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1854,
     "status": "ok",
     "timestamp": 1623118171979,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "VOSQPI5dDvh0"
   },
   "outputs": [],
   "source": [
    "# aplica normalização no dataframe\n",
    "df['texto_trat'] = df.texto.apply(fn_limpa_texto)\n",
    "\n",
    "# aplica a lematização no dataframe\n",
    "#df['texto_lemma'] = df.texto.apply(fn_lematiza_texto)\n",
    "\n",
    "# aplica a lematização dos verbos no dataframe\n",
    "#df['texto_lemma_verb'] = df.texto.apply(fn_lematiza_verb_texto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 44827,
     "status": "ok",
     "timestamp": 1623118216804,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "aTbJTpUeNyPy"
   },
   "outputs": [],
   "source": [
    "# Contagem de termos simples\n",
    "#vetor = CountVectorizer(ngram_range=(1,1))\n",
    "#vetor = CountVectorizer(ngram_range=(1,2)) #0.7801952252146301 - Contagem simples, texto tratado, uni e bi gramas, regressão\n",
    "#vetor = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "#vetor = CountVectorizer(ngram_range=(1,1), stop_words=stops_spacy)\n",
    "#vetor = CountVectorizer(ngram_range=(1,2), stop_words=stops_nltk)\n",
    "\n",
    "# Frequência de termos (TF)\n",
    "#vetor = TfidfVectorizer(ngram_range=(1,2), use_idf=False, norm='l1') #\n",
    "## vetor = TfidfVectorizer(ngram_range=(1,2), use_idf=False) #\n",
    "#vetor = TfidfVectorizer(ngram_range=(1,2), use_idf=False, stop_words=stops) #\n",
    "#vetor = TfidfVectorizer(ngram_range=(1,2), use_idf=False, stop_words=stops_nltk) #\n",
    "\n",
    "# Frequência de termos - term frequency–inverse document frequency (TF-IDF)\n",
    "#vetor = TfidfVectorizer(ngram_range=(1,1), use_idf=True, norm='l1') #\n",
    "#vetor = TfidfVectorizer(ngram_range=(1,2), use_idf=True, norm='l1') #\n",
    "#vetor = TfidfVectorizer(ngram_range=(1,2), use_idf=True) #0.7729036810537457 - TF-IDF, texto lematixado, uni e bi brama, Naive\n",
    "vetor = TfidfVectorizer(ngram_range=(1,2), use_idf=True, stop_words=stops) #\n",
    "#vetor = TfidfVectorizer(ngram_range=(1,2), use_idf=True, stop_words=stops_nltk) # lematixado\n",
    "\n",
    "# texto tratado\n",
    "\n",
    "vetor.fit(df.texto_trat)\n",
    "text_vect = vetor.transform(df.texto_trat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67789,
     "status": "ok",
     "timestamp": 1623118304824,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "0Bx03iB7pr9i",
    "outputId": "813df96b-d862-48e5-e89a-4fd855fd3cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44514, 2891076)\n",
      "0.8955408289340672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# separa as amostras de Treino (80%) e Texte (20%)\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "        text_vect, \n",
    "        df['sentimento'], \n",
    "        test_size = 0.2,\n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "# define e treina o modelo de classificação\n",
    "#modelo = LogisticRegression(random_state=42)\n",
    "#modelo = DecisionTreeClassifier(random_state=42)\n",
    "modelo = MultinomialNB()\n",
    "\n",
    "modelo = LogisticRegression(penalty = 'l2', C = 100, random_state=42)\n",
    "\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# valida a parformance\n",
    "y_prediction = modelo.predict(X_test)\n",
    "accuracy = accuracy_score(y_prediction, y_test)\n",
    "\n",
    "print(text_vect.shape)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1623118311509,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "E8hT4RxLB09c",
    "outputId": "70e6a4f0-359c-40a4-bc8f-6f0944bcce84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.55297580083781"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "f1_score(y_test, y_prediction, average='weighted')*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68SiMjcWqD_m"
   },
   "source": [
    "####**Validação do professor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T24EasckqG2I"
   },
   "source": [
    "Consolidar apenas os scripts do seu **modelo campeão**, desde o carregamento do dataframe, separação das amostras, tratamentos utilizados (funções, limpezas, etc.), criação dos objetos de vetorização dos textos e modelo treinado e outras implementações utilizadas no processo de desenvolvimento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "executionInfo": {
     "elapsed": 12468,
     "status": "error",
     "timestamp": 1623118230043,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "ZxqHA-XCrqsD",
    "outputId": "d723887b-39a7-45a8-c9e2-16eed213e54f"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-decded83d436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# texto tratado e verbos lematizado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvetor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexto_trat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#texto_lemma_verb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtext_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvetor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexto_trat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1836\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1837\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Modelo final\n",
    "\n",
    "# vetorização tf-idf com combinação de unigrama e bigrama\n",
    "vetor = TfidfVectorizer(ngram_range=(1,2), use_idf=True) # 0.7729036810537457\n",
    "\n",
    "# texto tratado e verbos lematizado\n",
    "vetor.fit(df.texto_trat)#texto_lemma_verb\n",
    "text_vect = vetor.transform(df.texto_trat)\n",
    "\n",
    "# separa as amostras de Treino (80%) e Texte (20%)\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "        text_vect, \n",
    "        df['sentimento'], \n",
    "        test_size = 0.2,\n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "# define e treina o modelo de classificação\n",
    "modelo = MultinomialNB()\n",
    "\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# valida a parformance\n",
    "y_prediction = modelo.predict(X_test)\n",
    "accuracy = accuracy_score(y_prediction, y_test)\n",
    "\n",
    "print(text_vect.shape)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "aborted",
     "timestamp": 1623118230039,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "BuJtvcfXo3J4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1623118230041,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "6ULYNH6-o3Hf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1623118230042,
     "user": {
      "displayName": "David Claro",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmyNbYDzdMvRKV9uPQEi0pzoJFFSi62sp51Us0V2s=s64",
      "userId": "06392505952487908464"
     },
     "user_tz": 180
    },
    "id": "3ClM-JTJo3FK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "history_visible": true,
   "name": "TrabalhoFinal_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
