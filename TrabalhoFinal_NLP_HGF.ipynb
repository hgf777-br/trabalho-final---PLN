{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbi6PDS9MYO"
      },
      "source": [
        "***Participantes (RM - NOME):***<br>\n",
        "339624 - Camila<br>\n",
        "339656 - Cleiton<br>\n",
        "340214 - Henrique<br>\n",
        "339708 - Roberto<br>\n",
        "340192 - Sergio<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xw6WhaNo4k3"
      },
      "source": [
        "## **Criar um classificador de sentimento aplicando técnicas de PLN**\n",
        "---\n",
        "\n",
        "Utilizando o dataset de revisões de filmes em português [1], criar um classificador de sentimentos que consiga um score na métrica F1 Score superior a 70%.\n",
        "\n",
        "Devem utilizar uma amostra de 20% e randon_state igual a 42 para testar as implementações e mensurar a métrica F1 Score (usar o parâmetro average = 'weighted') o restante dos dados devem ser utilizados para o treinamento (80%).\n",
        "\n",
        "Fique a vontade para testar os métodos de pré-processamento, abordagens, algoritmos e bibliotecas, mas explique e justifique suas decisões.\n",
        "O trabalho poderá ser feito em grupo de até 4 pessoas (mesmo grupo do Startup One).\n",
        "\n",
        "Separe a implementação do seu modelo campeão junto com a parte de validação/teste de forma que o professor consiga executar todo o pipeline do modelo campeão.\n",
        "\n",
        "Composição da nota:\n",
        "- 50% - Demonstrações das aplicações das técnicas de PLN (regras, pré-processamentos, tratamentos, variedade de modelos aplicados, etc.)\n",
        "- 50% - Baseado na performance obtida com o dataset de teste (conforme recomendação da amostra) no seu modelo campeão e na validação que o professor processar (Métrica F1 Score)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzhQpodBpRpX"
      },
      "source": [
        "[1] - https://dados-ml-pln.s3-sa-east-1.amazonaws.com/reviews-pt-br.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bibliotecas utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import wordcloud as wd\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.rslp import RSLPStemmer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, Perceptron, RidgeClassifier, SGDClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.layers import Dropout, Flatten, Conv1D, Embedding\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.utils import plot_model\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "import pydot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "sb.set_theme(context='notebook', style='whitegrid')\n",
        "\n",
        "nlp = spacy.load('pt_core_news_md')\n",
        "rslp = RSLPStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0\nTrue\nNum GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)\n",
        "#print(keras.__version__)\n",
        "print(tf.test.is_built_with_cuda())\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Funções externas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# função de Stemização completa do documento\n",
        "def stemmer_text(frase):\n",
        "  tokens = [rslp.stem(w) for w in frase.split()]\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "# função de lematização completa do documento\n",
        "def lemmatizer_text(frase):\n",
        "  doc = nlp(frase)\n",
        "  tokens = [w.lemma_ for w in doc]\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "# função de lematização somente para os verbos do documento\n",
        "def lemmatizer_verbs(frase):\n",
        "  doc = nlp(frase)\n",
        "  tokens = [w.lemma_ if w.pos_ == 'VERB' else w.text for w in doc]\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "# função de lematização apenas ADV e ADJ\n",
        "def lemmatizer_adv_adj(frase):\n",
        "  doc = nlp(frase)\n",
        "  tokens = [w.lemma_ for w in doc if (w.pos_ == 'ADV' or w.pos_ == 'ADJ')]\n",
        "  return \" \".join(tokens)  \n",
        "\n",
        "# função para retiral o plural das palavras em português\n",
        "def plural_singular(palavra):\n",
        "  r2 = r\"ses$|zes$|res$\"\n",
        "  r3 = r\"ões$|ães$\"\n",
        "  r4 = r\"ais$|éis$|óis$|uis$\"\n",
        "  r5 = r\"is$\"\n",
        "  r6 = r\"eis$\"\n",
        "  if palavra.endswith('s'):\n",
        "    if re.findall(r2, palavra):\n",
        "      return palavra[:-2]\n",
        "    if re.findall(r3, palavra):\n",
        "      return palavra[:-3] + \"ão\"\n",
        "    if re.findall(r4, palavra):\n",
        "      return palavra[:-2] + \"l\"\n",
        "    if re.findall(r5, palavra):\n",
        "      return palavra[:-1] + \"l\"\n",
        "    if re.findall(r6, palavra):\n",
        "      return palavra[:-1] + \"il\"\n",
        "    if palavra.endswith('ns'):\n",
        "      return palavra[:-2] + \"m\"\n",
        "    return palavra[:-1]\n",
        "  return palavra  \n",
        "\n",
        "# função para executar o pré processamento\n",
        "def pre(frase):\n",
        "  regex = r\"[`,.?:;!&\\\"]\"\n",
        "  palavras = frase.split()\n",
        "  palavras = [p.lower() for p in palavras if p not in stops]\n",
        "  palavras = [re.sub(regex, \"\", p) for p in palavras]\n",
        "  palavras = [p for p in palavras if len(p) >= 3]\n",
        "  palavras = [p for p in palavras if not p.isnumeric()]\n",
        "  palavras = [plural_singular(p) for p in palavras]\n",
        "\n",
        "  return \" \".join(palavras)\n",
        "\n",
        "# função para criar as bases de treino e teste vetorizadas, com Unigramas, Bigramas e Trigramas\n",
        "def criar_vetores(base_treino, base_teste, coluna):\n",
        "    nomes = ['CV ngram(1,1)', 'CV ngram(1,2)', 'CV ngram(1,3)', 'CV ngram(2,2)', 'CV ngram(2,3)', 'CV ngram(3,3)',\n",
        "                'TF ngram(1,1)', 'TF ngram(1,2)', 'TF ngram(1,3)', 'TF ngram(2,2)', 'TF ngram(2,3)', 'TF ngram(3,3)',]\n",
        "    vetores = []\n",
        "    idx = 0\n",
        "\n",
        "    for x in range(1,4):\n",
        "        for y in range(1,4): \n",
        "            if (y < x):\n",
        "                continue\n",
        "            vect = CountVectorizer(ngram_range=(x,y), stop_words = stops, min_df=10)\n",
        "            vect.fit(df_treino[coluna])\n",
        "            vect_treino = vect.transform(df_treino[coluna])\n",
        "            vect_teste = vect.transform(df_teste[coluna])\n",
        "            vetores.append([nomes[idx], vect_treino, vect_teste])\n",
        "            idx += 1\n",
        "\n",
        "    for x in range(1,4):\n",
        "        for y in range(1,4): \n",
        "            if (y < x):\n",
        "                continue\n",
        "            vect = TfidfVectorizer(ngram_range=(x,y), stop_words = stops, min_df=10)\n",
        "            vect.fit(df_treino['texto'])\n",
        "            vect_treino = vect.transform(df_treino[coluna])\n",
        "            vect_teste = vect.transform(df_teste[coluna])\n",
        "            vetores.append([nomes[idx], vect_treino, vect_teste])\n",
        "            idx += 1\n",
        "\n",
        "    return vetores\n",
        "\n",
        "def testar_modelo(modelo, base_treino, base_teste, vetores, coluna):\n",
        "    f1score = 0\n",
        "    nome = \"\"\n",
        "    for x in vetores:\n",
        "        modelo.fit(x[1], base_treino[coluna])\n",
        "        predito = modelo.predict(x[2])\n",
        "        f1 = f1_score(base_teste[coluna], predito, average='weighted')\n",
        "        print(x[0], f1)\n",
        "        if (f1score < f1):\n",
        "            f1score = f1\n",
        "            nome = x[0]\n",
        "\n",
        "    return (nome, f1score)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = \"Este fime é muito bom e especial\"\n",
        "\n",
        "d = nlp(t)\n",
        "for w in d:\n",
        "    print(w.text, w.pos_, w.lemma_, w.dep_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Carregando os Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMBI8SQtps1n"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./data/reviews-pt-br.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s__lBzDQwrcG",
        "outputId": "0bd3d84d-be60-4da3-c598-62f4a045b6c6"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue0nV0uVo3OZ"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyKC9Vhkp0BK"
      },
      "source": [
        "## Conferindo se temos dados nulos ou duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nze8UbKhosm9"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[df.duplicated()].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distribuição das respostas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.groupby('sentimento').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# em percentual\n",
        "round(df.groupby('sentimento').count().texto / df.shape[0] * 100, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FziwgqJmw9OD"
      },
      "outputs": [],
      "source": [
        "sb.countplot(x=df.sentimento)"
      ]
    },
    {
      "source": [
        "## Criando o conjunto de stopwords (NLTK + SPACY)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'acerca', 'ademais', 'adeus', 'agora', 'ainda', 'algo', 'algumas', 'alguns', 'ali', 'além', 'ambas', 'ambos', 'antes', 'ao', 'aos', 'apenas', 'apoia', 'apoio', 'apontar', 'após', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aqui', 'aquilo', 'as', 'assim', 'através', 'atrás', 'até', 'aí', 'baixo', 'bastante', 'bem', 'boa', 'bom', 'breve', 'cada', 'caminho', 'catorze', 'cedo', 'cento', 'certamente', 'certeza', 'cima', 'cinco', 'coisa', 'com', 'como', 'comprida', 'comprido', 'conhecida', 'conhecido', 'conselho', 'contra', 'contudo', 'corrente', 'cuja', 'cujo', 'custa', 'cá', 'da', 'daquela', 'daquele', 'dar', 'das', 'de', 'debaixo', 'dela', 'delas', 'dele', 'deles', 'demais', 'dentro', 'depois', 'des', 'desde', 'dessa', 'desse', 'desta', 'deste', 'deve', 'devem', 'deverá', 'dez', 'dezanove', 'dezasseis', 'dezassete', 'dezoito', 'diante', 'direita', 'disso', 'diz', 'dizem', 'dizer', 'do', 'dois', 'dos', 'doze', 'duas', 'dá', 'dão', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'embora', 'enquanto', 'entre', 'então', 'era', 'eram', 'essa', 'essas', 'esse', 'esses', 'esta', 'estado', 'estamos', 'estar', 'estará', 'estas', 'estava', 'estavam', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estiveste', 'estivestes', 'estivéramos', 'estivéssemos', 'estou', 'está', 'estás', 'estávamos', 'estão', 'eu', 'eventual', 'exemplo', 'falta', 'fará', 'favor', 'faz', 'fazeis', 'fazem', 'fazemos', 'fazer', 'fazes', 'fazia', 'faço', 'fez', 'fim', 'final', 'foi', 'fomos', 'for', 'fora', 'foram', 'forem', 'forma', 'formos', 'fosse', 'fossem', 'foste', 'fostes', 'fui', 'fôramos', 'fôssemos', 'geral', 'grande', 'grandes', 'grupo', 'haja', 'hajam', 'hajamos', 'havemos', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houveram', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houvermos', 'houverá', 'houverão', 'houveríamos', 'houvesse', 'houvessem', 'houvéramos', 'houvéssemos', 'há', 'hão', 'inclusive', 'iniciar', 'inicio', 'ir', 'irá', 'isso', 'isto', 'já', 'lado', 'lhe', 'lhes', 'ligado', 'local', 'logo', 'longe', 'lugar', 'lá', 'maior', 'maioria', 'maiorias', 'mais', 'mal', 'mas', 'me', 'meio', 'menor', 'menos', 'meses', 'mesmo', 'meu', 'meus', 'mil', 'minha', 'minhas', 'momento', 'muito', 'muitos', 'máximo', 'mês', 'na', 'nada', 'naquela', 'naquele', 'nas', 'nem', 'nenhuma', 'nessa', 'nesse', 'nesta', 'neste', 'no', 'nos', 'nossa', 'nossas', 'nosso', 'nossos', 'nova', 'novas', 'nove', 'novo', 'novos', 'num', 'numa', 'nunca', 'nuns', 'não', 'nível', 'nós', 'número', 'números', 'o', 'obrigada', 'obrigado', 'oitava', 'oitavo', 'oito', 'onde', 'ontem', 'onze', 'ora', 'os', 'ou', 'outra', 'outras', 'outros', 'para', 'parece', 'parte', 'partir', 'pegar', 'pela', 'pelas', 'pelo', 'pelos', 'perto', 'pode', 'podem', 'poder', 'poderá', 'podia', 'pois', 'ponto', 'pontos', 'por', 'porquanto', 'porque', 'porquê', 'portanto', 'porém', 'posição', 'possivelmente', 'posso', 'possível', 'pouca', 'pouco', 'povo', 'primeira', 'primeiro', 'próprio', 'próxima', 'próximo', 'puderam', 'pôde', 'põe', 'põem', 'quais', 'qual', 'qualquer', 'quando', 'quanto', 'quarta', 'quarto', 'quatro', 'que', 'quem', 'quer', 'querem', 'quero', 'questão', 'quieta', 'quieto', 'quinta', 'quinto', 'quinze', 'quê', 'relação', 'sabe', 'saber', 'se', 'segunda', 'segundo', 'sei', 'seis', 'seja', 'sejam', 'sejamos', 'sem', 'sempre', 'ser', 'serei', 'seremos', 'seria', 'seriam', 'será', 'serão', 'seríamos', 'sete', 'seu', 'seus', 'sexta', 'sexto', 'sim', 'sistema', 'sob', 'sobre', 'sois', 'somente', 'somos', 'sou', 'sua', 'suas', 'são', 'sétima', 'sétimo', 'só', 'tais', 'tal', 'talvez', 'também', 'tanta', 'tanto', 'tarde', 'te', 'tem', 'temos', 'tempo', 'tendes', 'tenha', 'tenham', 'tenhamos', 'tenho', 'tens', 'tentar', 'tentaram', 'tente', 'tentei', 'ter', 'terceira', 'terceiro', 'terei', 'teremos', 'teria', 'teriam', 'terá', 'terão', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tipo', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tiveste', 'tivestes', 'tivéramos', 'tivéssemos', 'toda', 'todas', 'todo', 'todos', 'treze', 'três', 'tu', 'tua', 'tuas', 'tudo', 'tão', 'tém', 'têm', 'tínhamos', 'um', 'uma', 'umas', 'uns', 'usa', 'usar', 'vai', 'vais', 'valor', 'veja', 'vem', 'vens', 'ver', 'vez', 'vezes', 'vinda', 'vindo', 'vinte', 'você', 'vocês', 'vos', 'vossa', 'vossas', 'vosso', 'vossos', 'vários', 'vão', 'vêm', 'vós', 'zero', 'à', 'às', 'área', 'é', 'éramos', 'és', 'último']\n"
          ]
        }
      ],
      "source": [
        "stopwords_nltk = nltk.corpus.stopwords.words('portuguese')\n",
        "stopwords_spacy = nlp.Defaults.stop_words\n",
        "stops = list(set(stopwords_spacy).union(stopwords_nltk))\n",
        "print(sorted(stops))"
      ]
    },
    {
      "source": [
        "# Analisando o conjunto total das palavras"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# exemplo de críticas no nosso dataframe\n",
        "print(df.texto[0])\n",
        "print(\"-\" * 40)\n",
        "print(df.texto[1536])\n",
        "print(\"-\" * 40)\n",
        "print(df.texto[8192])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# criando um texto completo com todas as críticas\n",
        "texto_completo = \" \".join([texto for texto in df.texto])\n",
        "texto_completo_pos = \" \".join([texto for texto in df[df.sentimento == 'pos'].texto])\n",
        "texto_completo_neg = \" \".join([texto for texto in df[df.sentimento == 'neg'].texto])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"QTD de todas as palvaras\", len(texto_completo.split()))\n",
        "print(\"QTD de todas as palvaras das críticas positivas\", len(texto_completo_pos.split()))\n",
        "print(\"QTD de todas as palvaras das críticas negativas\", len(texto_completo_neg.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# criando uma wordcloud de todas as palavra sem as stopwords\n",
        "wordcloud = wd.WordCloud(width = 3000, height = 2000, max_words=50, random_state=42, background_color='black', colormap='Blues', collocations=False, stopwords = stops).generate(texto_completo)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.title(\"Todas as palavras da Base de Dados retirando as StopWords\", fontdict={'fontsize':24})\n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\");\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# criando uma wordcloud de todas as palavra sem as stopwords\n",
        "wordcloud = wd.WordCloud(width = 3000, height = 2000, max_words=50, random_state=42, background_color='black', colormap='Blues', collocations=False, stopwords = stops).generate(texto_completo_pos)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.title(\"Todas as palavras POSITIVAS da Base de Dados retirando as StopWords\", fontdict={'fontsize':24})\n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\");\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# criando uma wordcloud de todas as palavra sem as stopwords\n",
        "wordcloud = wd.WordCloud(width = 3000, height = 2000, max_words=50, random_state=42, background_color='black', colormap='Blues', collocations=False, stopwords = stops).generate(texto_completo_neg)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.title(\"Todas as palavras NEGATIVAS da Base de Dados retirando as StopWords\", fontdict={'fontsize':24})\n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\");\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "## Criando uma função de pré processamento"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "palavras = texto_completo.split()\n",
        "len(palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# retirando as stopwords das palavras\n",
        "palavras = [p.lower() for p in palavras if p not in stops]\n",
        "len(palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# retirando pontuações das palavras\n",
        "regex = r\"[`,.?:;!&\\\"]\"\n",
        "palavras = [re.sub(regex, \"\", p) for p in palavras]\n",
        "len(palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#retirando palavras menores que 3 caracteres\n",
        "palavras = [p for p in palavras if len(p) >= 3]\n",
        "len(palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# retirando os números das palavras\n",
        "palavras = [p for p in palavras if not p.isnumeric()]\n",
        "len(palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#convertendo as palavras para o singular\n",
        "palavras = [plural_singular(p) for p in palavras]\n",
        "len(palavras)"
      ]
    },
    {
      "source": [
        "## Criamos uma função chamada \"pre\" que iremos usar em nossas análises dos modelos"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# Vamos analisar quais palavras são comuns as críticas negativas e positivas"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "todas_palavras_pos = texto_completo_pos.split()\n",
        "todas_palavras_pos = [pre(w) for w in todas_palavras_pos]\n",
        "todas_palavras_pos = [w for w in todas_palavras_pos if w != '']\n",
        "ctpp = Counter()\n",
        "ctpp.update(todas_palavras_pos)\n",
        "ctpp.most_common(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "todas_palavras_neg = texto_completo_neg.split()\n",
        "todas_palavras_neg = [pre(w) for w in todas_palavras_neg]\n",
        "todas_palavras_neg = [w for w in todas_palavras_neg if w != '']\n",
        "ctpn = Counter()\n",
        "ctpn.update(todas_palavras_neg)\n",
        "ctpn.most_common(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "p = ctpp.most_common()\n",
        "n = ctpn.most_common()\n",
        "r = [[x for x in p if x[0] == y[0]] for y in n]\n",
        "r = [x for x in r if len(x) != 0]\n",
        "len(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "palavras_comuns = [w[0][0] for w in r]\n",
        "palavras_comuns[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# criando uma wordcloud das palavras filtradas\n",
        "wordcloud = wd.WordCloud(width = 3000, height = 2000, max_words=50, random_state=42, background_color='black', colormap='Blues', collocations=False).fit_words(dict(ctp))\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.title(\"Todas as palavras da Base de Dados após o Pré Pocessamento\", fontdict={'fontsize':24})\n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\");\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "## Criando as colunas tratadas"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "#df['stem'] = df.texto.apply(stemmer_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%time\n",
        "#df['lemm'] = df.texto.apply(lemmatizer_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "#df['verb'] = df.texto.apply(lemmatizer_verbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "#df['pre'] = df.texto.apply(pre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df.to_csv(\"./data/criticas.csv\")\n",
        "df = pd.read_csv('../criticas.csv')\n",
        "df = df.drop(['Unnamed: 0', 'codigo'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               texto sentimento  \\\n",
              "0  Esse bocejo de pia de cozinha de orçamento mui...        neg   \n",
              "1  O Bravo parece indicar que o personagem princi...        neg   \n",
              "2  Durante a Guerra pela Independência do Sul, GE...        pos   \n",
              "3  É fora de questão que a verdadeira Anna Anders...        pos   \n",
              "4  Concordo totalmente com outro dos revisores aq...        neg   \n",
              "5  Obra-prima absoluta de um filme! Boa noite Mr....        pos   \n",
              "6  Embora a palavra megalmania seja muito usada p...        pos   \n",
              "7  Esta tem que ser a peça mais incrível de porca...        neg   \n",
              "8  Eu suponho que todas as piadas internas são o ...        neg   \n",
              "9  Se há um tema deste filme, é que as pessoas po...        pos   \n",
              "\n",
              "                                                stem  \\\n",
              "0  ess bocej de pia de co de orç muit baix é o ti...   \n",
              "1  o brav parec indic que o person principal, cla...   \n",
              "2  dur a guerr pel independ do sul, gener spanky ...   \n",
              "3  é for de quest que a verd ann anderson não era...   \n",
              "4  concord total com outr do revi aqu que fic sat...   \n",
              "5  obra-pr absolut de um filme! boa noit mr.tom r...   \n",
              "6  emb a palavr megalman sej muit us par descrev ...   \n",
              "7  est tem que ser a peç mais incr de porc cinema...   \n",
              "8  eu suponh que tod as pi intern são o que fez d...   \n",
              "9  se há um tem dest filme, é que as pesso pod li...   \n",
              "\n",
              "                                                lemm  \\\n",
              "0  Esse bocejar de pio de cozinhar de orçamentar ...   \n",
              "1  O Bravo parecer indicar que o personagem princ...   \n",
              "2  Durante o Guerra pelar Independência do Sul , ...   \n",
              "3  É ser de questão que o verdadeiro Anna Anderso...   \n",
              "4  Concordo totalmente com outro dos revisor aqui...   \n",
              "5  Obra-prima absoluto de um filmar ! Boa noite M...   \n",
              "6  Embora o palavra megalmania ser muito usar par...   \n",
              "7  Esta ter que ser o pedir mais incrível de porc...   \n",
              "8  Eu supor que todo o piar interno ser o que faz...   \n",
              "9  Se haver um temer dar filmar , ser que o pesso...   \n",
              "\n",
              "                                                verb  \\\n",
              "0  Esse bocejo de pia de cozinha de orçamento mui...   \n",
              "1  O Bravo parecer indicar que o personagem princ...   \n",
              "2  Durante a Guerra pela Independência do Sul , G...   \n",
              "3  É fora de questão que a verdadeira Anna Anders...   \n",
              "4  Concordo totalmente com outro dos revisores aq...   \n",
              "5  Obra-prima absoluta de um filme ! Boa noite Mr...   \n",
              "6  Embora a palavra megalmania seja muito usar pa...   \n",
              "7  Esta tem que ser a peça mais incrível de porca...   \n",
              "8  Eu supor que todas as piadas internas são o qu...   \n",
              "9  Se haver um tema deste filme , ser que as pess...   \n",
              "\n",
              "                                                 pre  \n",
              "0  esse bocejo pia cozinha orçamento filme feito ...  \n",
              "1  bravo indicar personagem principal claro coraj...  \n",
              "2  durante guerra independência sul general spank...  \n",
              "3  verdadeira anna anderson princesa anastasia al...  \n",
              "4  concordo totalmente outro revisor ficou satisf...  \n",
              "5  obra-prima absoluta filme boa noite mrtom rapi...  \n",
              "6  embora palavra megalmania usada descrever gene...  \n",
              "7  esta peça incrível porcaria cinematográfica as...  \n",
              "8  suponho piada interna munchie clássico cult pe...  \n",
              "9  tema filme pessoa lidar dificuldade imaginação...  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>texto</th>\n      <th>sentimento</th>\n      <th>stem</th>\n      <th>lemm</th>\n      <th>verb</th>\n      <th>pre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Esse bocejo de pia de cozinha de orçamento mui...</td>\n      <td>neg</td>\n      <td>ess bocej de pia de co de orç muit baix é o ti...</td>\n      <td>Esse bocejar de pio de cozinhar de orçamentar ...</td>\n      <td>Esse bocejo de pia de cozinha de orçamento mui...</td>\n      <td>esse bocejo pia cozinha orçamento filme feito ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>O Bravo parece indicar que o personagem princi...</td>\n      <td>neg</td>\n      <td>o brav parec indic que o person principal, cla...</td>\n      <td>O Bravo parecer indicar que o personagem princ...</td>\n      <td>O Bravo parecer indicar que o personagem princ...</td>\n      <td>bravo indicar personagem principal claro coraj...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Durante a Guerra pela Independência do Sul, GE...</td>\n      <td>pos</td>\n      <td>dur a guerr pel independ do sul, gener spanky ...</td>\n      <td>Durante o Guerra pelar Independência do Sul , ...</td>\n      <td>Durante a Guerra pela Independência do Sul , G...</td>\n      <td>durante guerra independência sul general spank...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>É fora de questão que a verdadeira Anna Anders...</td>\n      <td>pos</td>\n      <td>é for de quest que a verd ann anderson não era...</td>\n      <td>É ser de questão que o verdadeiro Anna Anderso...</td>\n      <td>É fora de questão que a verdadeira Anna Anders...</td>\n      <td>verdadeira anna anderson princesa anastasia al...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Concordo totalmente com outro dos revisores aq...</td>\n      <td>neg</td>\n      <td>concord total com outr do revi aqu que fic sat...</td>\n      <td>Concordo totalmente com outro dos revisor aqui...</td>\n      <td>Concordo totalmente com outro dos revisores aq...</td>\n      <td>concordo totalmente outro revisor ficou satisf...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Obra-prima absoluta de um filme! Boa noite Mr....</td>\n      <td>pos</td>\n      <td>obra-pr absolut de um filme! boa noit mr.tom r...</td>\n      <td>Obra-prima absoluto de um filmar ! Boa noite M...</td>\n      <td>Obra-prima absoluta de um filme ! Boa noite Mr...</td>\n      <td>obra-prima absoluta filme boa noite mrtom rapi...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Embora a palavra megalmania seja muito usada p...</td>\n      <td>pos</td>\n      <td>emb a palavr megalman sej muit us par descrev ...</td>\n      <td>Embora o palavra megalmania ser muito usar par...</td>\n      <td>Embora a palavra megalmania seja muito usar pa...</td>\n      <td>embora palavra megalmania usada descrever gene...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Esta tem que ser a peça mais incrível de porca...</td>\n      <td>neg</td>\n      <td>est tem que ser a peç mais incr de porc cinema...</td>\n      <td>Esta ter que ser o pedir mais incrível de porc...</td>\n      <td>Esta tem que ser a peça mais incrível de porca...</td>\n      <td>esta peça incrível porcaria cinematográfica as...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Eu suponho que todas as piadas internas são o ...</td>\n      <td>neg</td>\n      <td>eu suponh que tod as pi intern são o que fez d...</td>\n      <td>Eu supor que todo o piar interno ser o que faz...</td>\n      <td>Eu supor que todas as piadas internas são o qu...</td>\n      <td>suponho piada interna munchie clássico cult pe...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Se há um tema deste filme, é que as pessoas po...</td>\n      <td>pos</td>\n      <td>se há um tem dest filme, é que as pesso pod li...</td>\n      <td>Se haver um temer dar filmar , ser que o pesso...</td>\n      <td>Se haver um tema deste filme , ser que as pess...</td>\n      <td>tema filme pessoa lidar dificuldade imaginação...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stops2 = set(stops).union(palavras_comuns[:100])\n",
        "len(stops2)"
      ]
    },
    {
      "source": [
        "# Dividindo a base em treino e teste"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dividindo com 20% para o treino e random state = 42\n",
        "df_treino, df_teste = train_test_split(\n",
        "      df, \n",
        "      test_size = 0.2, \n",
        "      random_state = 42\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentimento\n",
              "neg    40.07\n",
              "pos    39.93\n",
              "Name: texto, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# distribuição das respostas do treino em %\n",
        "round(df_treino.groupby('sentimento').count() / df.shape[0] * 100, 2).texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentimento\n",
              "neg    10.04\n",
              "pos     9.96\n",
              "Name: texto, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# distribuição das respostas do teste em %\n",
        "round(df_teste.groupby('sentimento').count() / df.shape[0] * 100, 2).texto"
      ]
    },
    {
      "source": [
        "# Testando diversos modelos com a base total de palavras sem stopwords"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Criando os vetores com CountVectorizer e TfidfVectorizer"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall time: 4min 23s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# criando os vetores para Unigramas, Bigramas, Trigramas com CountVectorizer e TfidfVectorizer\n",
        "vetores = criar_vetores(df_treino, df_teste, 'texto')"
      ]
    },
    {
      "source": [
        "## Testando diversos modelos com os vetores criados"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.7334065607517617\n",
            "CV ngram(1,2) 0.7389972124839376\n",
            "CV ngram(1,3) 0.7416456410429769\n",
            "CV ngram(2,2) 0.6132194031871322\n",
            "CV ngram(2,3) 0.6143754350124869\n",
            "CV ngram(3,3) 0.440902916358259\n",
            "TF ngram(1,1) 0.7242198298707084\n",
            "TF ngram(1,2) 0.7260879021556159\n",
            "TF ngram(1,3) 0.7269344378693082\n",
            "TF ngram(2,2) 0.611408385276224\n",
            "TF ngram(2,3) 0.6152997395176724\n",
            "TF ngram(3,3) 0.4438377004527155\n",
            "\n",
            "Árvore de Decisão - CV ngram(1,3) - F1 Score: 74.16%\n",
            "Wall time: 3min 53s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Árvore de Decisão\n",
        "modelo = DecisionTreeClassifier(random_state=42, max_depth=50)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Árvore de Decisão - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8570091422646389\n",
            "CV ngram(1,2) 0.8580194411160453\n",
            "CV ngram(1,3) 0.8610559351736565\n",
            "CV ngram(2,2) 0.7738974149816571\n",
            "CV ngram(2,3) 0.7747957289121654\n",
            "CV ngram(3,3) 0.5939805367253124\n",
            "TF ngram(1,1) 0.8527194695341529\n",
            "TF ngram(1,2) 0.8548498155717084\n",
            "TF ngram(1,3) 0.8569934729963394\n",
            "TF ngram(2,2) 0.7777573321458281\n",
            "TF ngram(2,3) 0.7803476027605121\n",
            "TF ngram(3,3) 0.5922386561501604\n",
            "\n",
            "Random Forest - CV ngram(1,3) - F1 Score: 86.11%\n",
            "Wall time: 6min 19s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Random Forest\n",
        "modelo = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Random Forest - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8113072237344358\n",
            "CV ngram(1,2) 0.8119893681619048\n",
            "CV ngram(1,3) 0.8119893681619048\n",
            "CV ngram(2,2) 0.6631463063565599\n",
            "CV ngram(2,3) 0.6630078264083823\n",
            "CV ngram(3,3) 0.46169500243275796\n",
            "TF ngram(1,1) 0.8087066011318896\n",
            "TF ngram(1,2) 0.8060494093349827\n",
            "TF ngram(1,3) 0.8055787156701836\n",
            "TF ngram(2,2) 0.668940166492079\n",
            "TF ngram(2,3) 0.6717980213185778\n",
            "TF ngram(3,3) 0.46618350077771004\n",
            "\n",
            "AdaBoost - CV ngram(1,2) - F1 Score: 81.2%\n",
            "Wall time: 3min 46s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo AdaBoost\n",
        "modelo = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"AdaBoost - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.7829529863109658\n",
            "CV ngram(1,2) 0.7822625447604084\n",
            "CV ngram(1,3) 0.7840829303681472\n",
            "CV ngram(2,2) 0.7470878093253949\n",
            "CV ngram(2,3) 0.7506578822171368\n",
            "CV ngram(3,3) 0.5922569249811895\n",
            "TF ngram(1,1) 0.7935963307402973\n",
            "TF ngram(1,2) 0.787549924702114\n",
            "TF ngram(1,3) 0.793389110907307\n",
            "TF ngram(2,2) 0.749359985803243\n",
            "TF ngram(2,3) 0.7512825374870906\n",
            "TF ngram(3,3) 0.592533692930128\n",
            "\n",
            "Bagging - TF ngram(1,1) - F1 Score: 79.36%\n",
            "Wall time: 27min 30s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Bagging Classifier\n",
        "modelo = BaggingClassifier(n_estimators=20, random_state=42, n_jobs=-1)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Bagging - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8708200533848273\n",
            "CV ngram(1,2) 0.8759927433580862\n",
            "CV ngram(1,3) 0.8758795252960768\n",
            "CV ngram(2,2) 0.7811582020103887\n",
            "CV ngram(2,3) 0.7781320574766459\n",
            "CV ngram(3,3) 0.5925747779938603\n",
            "TF ngram(1,1) 0.8709188943514428\n",
            "TF ngram(1,2) 0.8750847464964969\n",
            "TF ngram(1,3) 0.8756461176623055\n",
            "TF ngram(2,2) 0.7963599859499786\n",
            "TF ngram(2,3) 0.7935512034983379\n",
            "TF ngram(3,3) 0.5923588709500518\n",
            "\n",
            "Extra Trees Regressor - CV ngram(1,2) - F1 Score: 87.6%\n",
            "Wall time: 9min 20s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Extra Trees Regressor\n",
        "modelo = ExtraTreesClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Extra Trees Regressor - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8131037840926045\n",
            "CV ngram(1,2) 0.8154661088177287\n",
            "CV ngram(1,3) 0.8136564531371888\n",
            "CV ngram(2,2) 0.703771331017916\n",
            "CV ngram(2,3) 0.7053132711254233\n",
            "CV ngram(3,3) 0.5000896766537951\n",
            "TF ngram(1,1) 0.8132784816099118\n",
            "TF ngram(1,2) 0.8164773941162936\n",
            "TF ngram(1,3) 0.814008556621088\n",
            "TF ngram(2,2) 0.7067345728742199\n",
            "TF ngram(2,3) 0.7063127642730296\n",
            "TF ngram(3,3) 0.500132627555598\n",
            "\n",
            "Gradient Boosting - TF ngram(1,2) - F1 Score: 81.65%\n",
            "Wall time: 13min 26s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Gradient Boosting\n",
        "modelo = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Gradient Boosting - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\hgf77\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "CV ngram(1,1) 0.8701556226033939\n",
            "C:\\Users\\hgf77\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "CV ngram(1,2) 0.8791421062562188\n",
            "C:\\Users\\hgf77\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "CV ngram(1,3) 0.8792543850650043\n",
            "CV ngram(2,2) 0.8180140954546817\n",
            "CV ngram(2,3) 0.8157729065954192\n",
            "CV ngram(3,3) 0.5957558146768905\n",
            "TF ngram(1,1) 0.8809187826450897\n",
            "TF ngram(1,2) 0.889348947719393\n",
            "TF ngram(1,3) 0.8890110656229786\n",
            "TF ngram(2,2) 0.8275236995216966\n",
            "TF ngram(2,3) 0.8272959671164328\n",
            "TF ngram(3,3) 0.5968758108946228\n",
            "\n",
            "Regressão Logística - TF ngram(1,2) - F1 Score: 88.93%\n",
            "Wall time: 2min 51s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Regressão Logística\n",
        "modelo = LogisticRegression(solver='saga', max_iter=1000, random_state=42, n_jobs=-1)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Regressão Logística - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8456691927963159\n",
            "CV ngram(1,2) 0.8662239768753464\n",
            "CV ngram(1,3) 0.8673472012508682\n",
            "CV ngram(2,2) 0.7765912918116423\n",
            "CV ngram(2,3) 0.7743456179758983\n",
            "CV ngram(3,3) 0.5423155547597628\n",
            "TF ngram(1,1) 0.8507237969132462\n",
            "TF ngram(1,2) 0.8715043635253087\n",
            "TF ngram(1,3) 0.8717270286280259\n",
            "TF ngram(2,2) 0.7825432505960539\n",
            "TF ngram(2,3) 0.7844517846905066\n",
            "TF ngram(3,3) 0.5502119859308695\n",
            "\n",
            "Passive Agressive - TF ngram(1,3) - F1 Score: 87.17%\n",
            "Wall time: 3.44 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Passive Agressive\n",
        "modelo = PassiveAggressiveClassifier(random_state=42, n_jobs=-1)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Passive Agressive - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.7585067593071052\n",
            "CV ngram(1,2) 0.7477234170084225\n",
            "CV ngram(1,3) 0.7525534828089833\n",
            "CV ngram(2,2) 0.7399582036362565\n",
            "CV ngram(2,3) 0.7370462071476761\n",
            "CV ngram(3,3) 0.5925886471995548\n",
            "TF ngram(1,1) 0.8773200856296003\n",
            "TF ngram(1,2) 0.8891242755320212\n",
            "TF ngram(1,3) 0.8883345753860943\n",
            "TF ngram(2,2) 0.8149814872678168\n",
            "TF ngram(2,3) 0.8167834743557018\n",
            "TF ngram(3,3) 0.595673890235394\n",
            "\n",
            "Ridge - TF ngram(1,2) - F1 Score: 88.91%\n",
            "Wall time: 23 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Ridge\n",
        "modelo = RidgeClassifier(solver='sparse_cg', random_state=42)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Ridge - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8548745222099186\n",
            "CV ngram(1,2) 0.8677979087343012\n",
            "CV ngram(1,3) 0.8672275315382145\n",
            "CV ngram(2,2) 0.807559686093767\n",
            "CV ngram(2,3) 0.8044276726022707\n",
            "CV ngram(3,3) 0.5798838824002565\n",
            "TF ngram(1,1) 0.8816760564524672\n",
            "TF ngram(1,2) 0.8879717331315884\n",
            "TF ngram(1,3) 0.8878598659072925\n",
            "TF ngram(2,2) 0.8247457475321708\n",
            "TF ngram(2,3) 0.8234283422807653\n",
            "TF ngram(3,3) 0.5883600913938836\n",
            "\n",
            "SGD - TF ngram(1,2) - F1 Score: 88.8%\n",
            "Wall time: 2.75 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo SGD\n",
        "modelo = SGDClassifier(random_state=42, n_jobs=-1)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"SGD - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8561785530659148\n",
            "CV ngram(1,2) 0.8746468123072807\n",
            "CV ngram(1,3) 0.8753162962608646\n",
            "CV ngram(2,2) 0.8410494096777025\n",
            "CV ngram(2,3) 0.8381562881984885\n",
            "CV ngram(3,3) 0.5979569271742787\n",
            "TF ngram(1,1) 0.8561785530659148\n",
            "TF ngram(1,2) 0.8746468123072807\n",
            "TF ngram(1,3) 0.8753162962608646\n",
            "TF ngram(2,2) 0.8410494096777025\n",
            "TF ngram(2,3) 0.8381562881984885\n",
            "TF ngram(3,3) 0.5979569271742787\n",
            "\n",
            "Naive Bayes Bernoulli - CV ngram(1,3) - F1 Score: 87.53%\n",
            "Wall time: 1.66 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Naive Bayes Bernoulli\n",
        "modelo = BernoulliNB()\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Naive Bayes Bernoulli - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8541920098423615\n",
            "CV ngram(1,2) 0.8710538417137474\n",
            "CV ngram(1,3) 0.8718351898036343\n",
            "CV ngram(2,2) 0.8401366622752207\n",
            "CV ngram(2,3) 0.8396681025173632\n",
            "CV ngram(3,3) 0.5734863253834386\n",
            "TF ngram(1,1) 0.8618447927514207\n",
            "TF ngram(1,2) 0.8746325936041583\n",
            "TF ngram(1,3) 0.8748477005896964\n",
            "TF ngram(2,2) 0.8393536418952174\n",
            "TF ngram(2,3) 0.8395563782718626\n",
            "TF ngram(3,3) 0.5747816408564258\n",
            "\n",
            "Naive Bayes Complement - TF ngram(1,3) - F1 Score: 87.48%\n",
            "Wall time: 1.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Naive Bayes Complement\n",
        "modelo = ComplementNB()\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Naive Bayes Complement - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8540793419186162\n",
            "CV ngram(1,2) 0.8710538417137474\n",
            "CV ngram(1,3) 0.871723063743025\n",
            "CV ngram(2,2) 0.8400257852209359\n",
            "CV ngram(2,3) 0.8395563782718626\n",
            "CV ngram(3,3) 0.5726916420207792\n",
            "TF ngram(1,1) 0.8613950353813321\n",
            "TF ngram(1,2) 0.8747465261210724\n",
            "TF ngram(1,3) 0.8746261871797448\n",
            "TF ngram(2,2) 0.839021536888149\n",
            "TF ngram(2,3) 0.8397859898224573\n",
            "TF ngram(3,3) 0.5748707653722428\n",
            "\n",
            "Naive Bayes Multinomial - TF ngram(1,2) - F1 Score: 87.47%\n",
            "Wall time: 1.48 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Naive Bayes Multinomial\n",
        "modelo = MultinomialNB()\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Naive Bayes Multinomial - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\hgf77\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
            "CV ngram(1,1) 0.8427503134500534\n",
            "C:\\Users\\hgf77\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
            "CV ngram(1,2) 0.8593696786901172\n",
            "C:\\Users\\hgf77\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
            "CV ngram(1,3) 0.8592543708124906\n",
            "CV ngram(2,2) 0.7835525602249629\n",
            "CV ngram(2,3) 0.7817598993091702\n",
            "CV ngram(3,3) 0.5928683562102969\n",
            "TF ngram(1,1) 0.8785747941960814\n",
            "TF ngram(1,2) 0.8899243167399011\n",
            "TF ngram(1,3) 0.8908224226768418\n",
            "TF ngram(2,2) 0.8137665791374764\n",
            "TF ngram(2,3) 0.8117400847029504\n",
            "TF ngram(3,3) 0.595624170891155\n",
            "\n",
            "SVM Linear - TF ngram(1,3) - F1 Score: 89.08%\n",
            "Wall time: 35.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Linear SVM\n",
        "modelo = LinearSVC(random_state=42, max_iter=2000)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"SVM Linear - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "source": [
        "# Testando os modelos com a base stematizada sem stopwords"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall time: 4min 35s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# criando os vetores para Unigramas, Bigramas, Trigramas com CountVectorizer e TfidfVectorizer\n",
        "vetores = criar_vetores(df_treino, df_teste, 'stem')"
      ]
    },
    {
      "source": [
        "## apenas modelos com mais de 85% no passo anterior e tempo inferior a 5 minutos"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\hgf77\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "CV ngram(1,1) 0.8706031610913865\n",
            "C:\\Users\\hgf77\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "CV ngram(1,2) 0.8840837095164026\n",
            "C:\\Users\\hgf77\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n",
            "CV ngram(1,3) 0.8848676248811861\n",
            "CV ngram(2,2) 0.8376838758268568\n",
            "CV ngram(2,3) 0.8376805377601975\n",
            "CV ngram(3,3) 0.7150445510051537\n",
            "TF ngram(1,1) 0.8225359724241443\n",
            "TF ngram(1,2) 0.8231228787398793\n",
            "TF ngram(1,3) 0.8235723758714172\n",
            "TF ngram(2,2) 0.6149723776050718\n",
            "TF ngram(2,3) 0.6150512107052606\n",
            "TF ngram(3,3) 0.3748284219463762\n",
            "\n",
            "Regressão Logística - CV ngram(1,3) - F1 Score: 88.49%\n",
            "Wall time: 3min 53s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Regressão Logística\n",
        "modelo = LogisticRegression(solver='saga', max_iter=1000, random_state=42, n_jobs=-1)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Regressão Logística - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8447715624515739\n",
            "CV ngram(1,2) 0.8720642271552428\n",
            "CV ngram(1,3) 0.8737499306282133\n",
            "CV ngram(2,2) 0.8154524423713615\n",
            "CV ngram(2,3) 0.8211824042196831\n",
            "CV ngram(3,3) 0.670711521878216\n",
            "TF ngram(1,1) 0.7824273705555479\n",
            "TF ngram(1,2) 0.7946762938640037\n",
            "TF ngram(1,3) 0.7944521435751063\n",
            "TF ngram(2,2) 0.594318686498427\n",
            "TF ngram(2,3) 0.5929128015931685\n",
            "TF ngram(3,3) 0.35665806113545495\n",
            "\n",
            "Passive Agressive - CV ngram(1,3) - F1 Score: 87.37%\n",
            "Wall time: 4.73 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Passive Agressive\n",
        "modelo = PassiveAggressiveClassifier(random_state=42, n_jobs=-1)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Passive Agressive - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.7800723288078767\n",
            "CV ngram(1,2) 0.7776023312377306\n",
            "CV ngram(1,3) 0.7891719816823664\n",
            "CV ngram(2,2) 0.7305320606980007\n",
            "CV ngram(2,3) 0.7366007736035252\n",
            "CV ngram(3,3) 0.6931774787857015\n",
            "TF ngram(1,1) 0.8310286072770471\n",
            "TF ngram(1,2) 0.8360933383775296\n",
            "TF ngram(1,3) 0.8357547960627466\n",
            "TF ngram(2,2) 0.610473749539492\n",
            "TF ngram(2,3) 0.6107622720271955\n",
            "TF ngram(3,3) 0.3740804219182571\n",
            "\n",
            "Ridge - TF ngram(1,2) - F1 Score: 83.61%\n",
            "Wall time: 39.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Ridge\n",
        "modelo = RidgeClassifier(solver='sparse_cg', random_state=42)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Ridge - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8593501815753932\n",
            "CV ngram(1,2) 0.8739129241828761\n",
            "CV ngram(1,3) 0.8732718898722627\n",
            "CV ngram(2,2) 0.816242022298419\n",
            "CV ngram(2,3) 0.8228637616258457\n",
            "CV ngram(3,3) 0.7094704488293562\n",
            "TF ngram(1,1) 0.8164689388996369\n",
            "TF ngram(1,2) 0.819060757664068\n",
            "TF ngram(1,3) 0.8184998012692756\n",
            "TF ngram(2,2) 0.612289585227973\n",
            "TF ngram(2,3) 0.6125923903751722\n",
            "TF ngram(3,3) 0.373464245097864\n",
            "\n",
            "SGD - CV ngram(1,2) - F1 Score: 87.39%\n",
            "Wall time: 3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo SGD\n",
        "modelo = SGDClassifier(random_state=42, n_jobs=-1)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"SGD - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.853279813727806\n",
            "CV ngram(1,2) 0.8726032412689158\n",
            "CV ngram(1,3) 0.8748100562187059\n",
            "CV ngram(2,2) 0.8460315157127096\n",
            "CV ngram(2,3) 0.8445608675521092\n",
            "CV ngram(3,3) 0.7242011371467321\n",
            "TF ngram(1,1) 0.8198725818996544\n",
            "TF ngram(1,2) 0.8152285233285822\n",
            "TF ngram(1,3) 0.8152326385312899\n",
            "TF ngram(2,2) 0.6169621086941445\n",
            "TF ngram(2,3) 0.6161082603679379\n",
            "TF ngram(3,3) 0.37557593287108426\n",
            "\n",
            "Naive Bayes Bernoulli - CV ngram(1,3) - F1 Score: 87.48%\n",
            "Wall time: 1.52 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Naive Bayes Bernoulli\n",
        "modelo = BernoulliNB()\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Naive Bayes Bernoulli - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8475371319702153\n",
            "CV ngram(1,2) 0.8686958159758584\n",
            "CV ngram(1,3) 0.8714971557859723\n",
            "CV ngram(2,2) 0.8591156073972187\n",
            "CV ngram(2,3) 0.8594120043129988\n",
            "CV ngram(3,3) 0.7150760022360954\n",
            "TF ngram(1,1) 0.8136591938472498\n",
            "TF ngram(1,2) 0.8143208959389149\n",
            "TF ngram(1,3) 0.8145369026247485\n",
            "TF ngram(2,2) 0.618029123817186\n",
            "TF ngram(2,3) 0.6168547481595895\n",
            "TF ngram(3,3) 0.37369042341230274\n",
            "\n",
            "Naive Bayes Complement - CV ngram(1,3) - F1 Score: 87.15%\n",
            "Wall time: 1.35 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Naive Bayes Complement\n",
        "modelo = ComplementNB()\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Naive Bayes Complement - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV ngram(1,1) 0.8473112660817483\n",
            "CV ngram(1,2) 0.868583560494826\n",
            "CV ngram(1,3) 0.8714971557859723\n",
            "CV ngram(2,2) 0.8590047111285145\n",
            "CV ngram(2,3) 0.8595265442310434\n",
            "CV ngram(3,3) 0.715056211064448\n",
            "TF ngram(1,1) 0.8130790345962262\n",
            "TF ngram(1,2) 0.8137398018967721\n",
            "TF ngram(1,3) 0.8138439492218046\n",
            "TF ngram(2,2) 0.6180576030656999\n",
            "TF ngram(2,3) 0.617623906023027\n",
            "TF ngram(3,3) 0.37369042341230274\n",
            "\n",
            "Naive Bayes Multinomial - CV ngram(1,3) - F1 Score: 87.15%\n",
            "Wall time: 1.33 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Naive Bayes Multinomial\n",
        "modelo = MultinomialNB()\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"Naive Bayes Multinomial - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\hgf77\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
            "CV ngram(1,1) 0.8427503015472957\n",
            "C:\\Users\\hgf77\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
            "CV ngram(1,2) 0.8681338243394326\n",
            "C:\\Users\\hgf77\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
            "CV ngram(1,3) 0.8676849865494912\n",
            "CV ngram(2,2) 0.8082677036133574\n",
            "CV ngram(2,3) 0.8142200102456199\n",
            "CV ngram(3,3) 0.7003582313157055\n",
            "TF ngram(1,1) 0.8324071297488379\n",
            "TF ngram(1,2) 0.8368966062975588\n",
            "TF ngram(1,3) 0.8364479335806516\n",
            "TF ngram(2,2) 0.611141878648815\n",
            "TF ngram(2,3) 0.6095122075614553\n",
            "TF ngram(3,3) 0.37385064541276897\n",
            "\n",
            "SVM Linear - CV ngram(1,2) - F1 Score: 86.81%\n",
            "Wall time: 48.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# modelo Linear SVM\n",
        "modelo = LinearSVC(random_state=42, max_iter=2000)\n",
        "# calculo com todos os vetores.\n",
        "f1score = testar_modelo(modelo, df_treino, df_teste, vetores, 'sentimento')\n",
        "\n",
        "print()\n",
        "print(f\"SVM Linear - {f1score[0]} - F1 Score: {round(f1score[1]*100,2)}%\")"
      ]
    },
    {
      "source": [
        "# Testando os modelos com a base lematizada sem stopwords"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# MLP"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               texto sentimento  \\\n",
              "0  Esse bocejo de pia de cozinha de orçamento mui...        neg   \n",
              "1  O Bravo parece indicar que o personagem princi...        neg   \n",
              "2  Durante a Guerra pela Independência do Sul, GE...        pos   \n",
              "3  É fora de questão que a verdadeira Anna Anders...        pos   \n",
              "4  Concordo totalmente com outro dos revisores aq...        neg   \n",
              "\n",
              "                                                stem  \\\n",
              "0  ess bocej de pia de co de orç muit baix é o ti...   \n",
              "1  o brav parec indic que o person principal, cla...   \n",
              "2  dur a guerr pel independ do sul, gener spanky ...   \n",
              "3  é for de quest que a verd ann anderson não era...   \n",
              "4  concord total com outr do revi aqu que fic sat...   \n",
              "\n",
              "                                                lemm  \\\n",
              "0  Esse bocejar de pio de cozinhar de orçamentar ...   \n",
              "1  O Bravo parecer indicar que o personagem princ...   \n",
              "2  Durante o Guerra pelar Independência do Sul , ...   \n",
              "3  É ser de questão que o verdadeiro Anna Anderso...   \n",
              "4  Concordo totalmente com outro dos revisor aqui...   \n",
              "\n",
              "                                                verb  \\\n",
              "0  Esse bocejo de pia de cozinha de orçamento mui...   \n",
              "1  O Bravo parecer indicar que o personagem princ...   \n",
              "2  Durante a Guerra pela Independência do Sul , G...   \n",
              "3  É fora de questão que a verdadeira Anna Anders...   \n",
              "4  Concordo totalmente com outro dos revisores aq...   \n",
              "\n",
              "                                                 pre  \n",
              "0  esse bocejo pia cozinha orçamento filme feito ...  \n",
              "1  bravo indicar personagem principal claro coraj...  \n",
              "2  durante guerra independência sul general spank...  \n",
              "3  verdadeira anna anderson princesa anastasia al...  \n",
              "4  concordo totalmente outro revisor ficou satisf...  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>texto</th>\n      <th>sentimento</th>\n      <th>stem</th>\n      <th>lemm</th>\n      <th>verb</th>\n      <th>pre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Esse bocejo de pia de cozinha de orçamento mui...</td>\n      <td>neg</td>\n      <td>ess bocej de pia de co de orç muit baix é o ti...</td>\n      <td>Esse bocejar de pio de cozinhar de orçamentar ...</td>\n      <td>Esse bocejo de pia de cozinha de orçamento mui...</td>\n      <td>esse bocejo pia cozinha orçamento filme feito ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>O Bravo parece indicar que o personagem princi...</td>\n      <td>neg</td>\n      <td>o brav parec indic que o person principal, cla...</td>\n      <td>O Bravo parecer indicar que o personagem princ...</td>\n      <td>O Bravo parecer indicar que o personagem princ...</td>\n      <td>bravo indicar personagem principal claro coraj...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Durante a Guerra pela Independência do Sul, GE...</td>\n      <td>pos</td>\n      <td>dur a guerr pel independ do sul, gener spanky ...</td>\n      <td>Durante o Guerra pelar Independência do Sul , ...</td>\n      <td>Durante a Guerra pela Independência do Sul , G...</td>\n      <td>durante guerra independência sul general spank...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>É fora de questão que a verdadeira Anna Anders...</td>\n      <td>pos</td>\n      <td>é for de quest que a verd ann anderson não era...</td>\n      <td>É ser de questão que o verdadeiro Anna Anderso...</td>\n      <td>É fora de questão que a verdadeira Anna Anders...</td>\n      <td>verdadeira anna anderson princesa anastasia al...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Concordo totalmente com outro dos revisores aq...</td>\n      <td>neg</td>\n      <td>concord total com outr do revi aqu que fic sat...</td>\n      <td>Concordo totalmente com outro dos revisor aqui...</td>\n      <td>Concordo totalmente com outro dos revisores aq...</td>\n      <td>concordo totalmente outro revisor ficou satisf...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               texto  \\\n",
              "0  Esse bocejo de pia de cozinha de orçamento mui...   \n",
              "1  O Bravo parece indicar que o personagem princi...   \n",
              "2  Durante a Guerra pela Independência do Sul, GE...   \n",
              "3  É fora de questão que a verdadeira Anna Anders...   \n",
              "4  Concordo totalmente com outro dos revisores aq...   \n",
              "\n",
              "                                                 pre  target  \n",
              "0  esse bocejo pia cozinha orçamento filme feito ...       0  \n",
              "1  bravo indicar personagem principal claro coraj...       0  \n",
              "2  durante guerra independência sul general spank...       1  \n",
              "3  verdadeira anna anderson princesa anastasia al...       1  \n",
              "4  concordo totalmente outro revisor ficou satisf...       0  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>texto</th>\n      <th>pre</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Esse bocejo de pia de cozinha de orçamento mui...</td>\n      <td>esse bocejo pia cozinha orçamento filme feito ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>O Bravo parece indicar que o personagem princi...</td>\n      <td>bravo indicar personagem principal claro coraj...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Durante a Guerra pela Independência do Sul, GE...</td>\n      <td>durante guerra independência sul general spank...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>É fora de questão que a verdadeira Anna Anders...</td>\n      <td>verdadeira anna anderson princesa anastasia al...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Concordo totalmente com outro dos revisores aq...</td>\n      <td>concordo totalmente outro revisor ficou satisf...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df_mlp = df[['texto', 'pre']].copy()\n",
        "df_mlp['target'] = [0 if x == 'neg' else 1 for x in df.sentimento]\n",
        "df_mlp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 44514 entries, 0 to 44513\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   texto   44514 non-null  object\n 1   pre     44514 non-null  object\n 2   target  44514 non-null  int64 \ndtypes: int64(1), object(2)\nmemory usage: 1.0+ MB\n"
          ]
        }
      ],
      "source": [
        "# dividindo com 20% para o treino e random state = 42\n",
        "df_treino_mlp, df_teste_mlp = train_test_split(\n",
        "      df_mlp, \n",
        "      test_size = 0.2, \n",
        "      random_state = 42\n",
        "  )\n",
        "df_mlp.info()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "target\n",
              "0    40.07\n",
              "1    39.93\n",
              "Name: texto, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# distribuição das respostas do treino em %\n",
        "round(df_treino_mlp.groupby('target').count() / df_mlp.shape[0] * 100, 2).texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "target\n",
              "0    10.04\n",
              "1     9.96\n",
              "Name: texto, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# distribuição das respostas do teste em %\n",
        "round(df_teste_mlp.groupby('target').count() / df_mlp.shape[0] * 100, 2).texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts(df_mlp.pre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t.document_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoded_docs = t.texts_to_matrix(df_mlp.pre, mode='count')\n",
        "encoded_docs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vetorização em unigramas + Bigramas com CountVectorizer e sem stopwords\n",
        "vect_MLP = CountVectorizer(ngram_range=(1,2), stop_words=stops, min_df=20) \n",
        "#vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect_MLP.fit(df_treino_mlp.pre)\n",
        "vect_treino_MLP = vect_MLP.transform(df_treino_mlp.pre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35611, 22691)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "X = vect_treino_MLP.toarray()\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35611, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "y = df_treino_mlp.target.values.reshape(-1,1)\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Modelo da Rede MLP\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=X.shape[1], output_dim=128, input_length=X.shape[1]))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Conv1D(128, 7, padding='valid', activation='relu', strides=3))\n",
        "model.add(Conv1D(128, 7, padding='valid', activation='relu', strides=3))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "#model.add(Dense(50, activation='relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 44697, 128)        5721216   \n_________________________________________________________________\ndropout (Dropout)            (None, 44697, 128)        0         \n_________________________________________________________________\nconv1d (Conv1D)              (None, 14897, 128)        114816    \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 4964, 128)         114816    \n_________________________________________________________________\nglobal_max_pooling1d (Global (None, 128)               0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               16512     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 5,967,489\nTrainable params: 5,967,489\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
          ]
        }
      ],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "plot_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "381/891 [===========>..................] - ETA: 3:47 - loss: 0.6937 - accuracy: 0.4973"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#Processo de treinamento com 80% dos dados\n",
        "history  = model.fit(X, y, epochs=5, validation_split=0.2, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_teste = vect.transform(df_teste_mlp.pre)\n",
        "X_teste = vect_teste.toarray()\n",
        "X_teste.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_teste = df_teste_mlp.target.values.reshape(-1,1)\n",
        "y_teste.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Processo de teste com 30% dos dados que não foram utilizados no treinamento\n",
        "y_pred_classes  = model.predict_classes(X_teste)\n",
        "y_pred_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores = model.evaluate(X_teste, y_teste)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68SiMjcWqD_m"
      },
      "source": [
        "#### **Validação do professor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T24EasckqG2I"
      },
      "source": [
        "Consolidar apenas os scripts do seu **modelo campeão**, desde o carregamento do dataframe, separação das amostras, tratamentos utilizados (funções, limpezas, etc.), criação dos objetos de vetorização dos textos e modelo treinado e outras implementações utilizadas no processo de desenvolvimento do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFA-CYfawkEJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TemplateTrabalhoFinal-NLP.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "c24c9dacf042e5cf8b743bae11b2cef3a95983df3bc5153773d9ffef1d5207d2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "metadata": {
      "interpreter": {
        "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}