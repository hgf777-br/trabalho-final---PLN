{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbi6PDS9MYO"
      },
      "source": [
        "***Participantes (RM - NOME):***<br>\n",
        "339624 - Camila<br>\n",
        "339656 - Cleiton<br>\n",
        "340214 - Henrique<br>\n",
        "339708 - Roberto<br>\n",
        "340192 - Sergio<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xw6WhaNo4k3"
      },
      "source": [
        "## **Criar um classificador de sentimento aplicando técnicas de PLN**\n",
        "---\n",
        "\n",
        "Utilizando o dataset de revisões de filmes em português [1], criar um classificador de sentimentos que consiga um score na métrica F1 Score superior a 70%.\n",
        "\n",
        "Devem utilizar uma amostra de 20% e randon_state igual a 42 para testar as implementações e mensurar a métrica F1 Score (usar o parâmetro average = 'weighted') o restante dos dados devem ser utilizados para o treinamento (80%).\n",
        "\n",
        "Fique a vontade para testar os métodos de pré-processamento, abordagens, algoritmos e bibliotecas, mas explique e justifique suas decisões.\n",
        "O trabalho poderá ser feito em grupo de até 4 pessoas (mesmo grupo do Startup One).\n",
        "\n",
        "Separe a implementação do seu modelo campeão junto com a parte de validação/teste de forma que o professor consiga executar todo o pipeline do modelo campeão.\n",
        "\n",
        "Composição da nota:\n",
        "- 50% - Demonstrações das aplicações das técnicas de PLN (regras, pré-processamentos, tratamentos, variedade de modelos aplicados, etc.)\n",
        "- 50% - Baseado na performance obtida com o dataset de teste (conforme recomendação da amostra) no seu modelo campeão e na validação que o professor processar (Métrica F1 Score)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzhQpodBpRpX"
      },
      "source": [
        "[1] - https://dados-ml-pln.s3-sa-east-1.amazonaws.com/reviews-pt-br.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bibliotecas utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import wordcloud as wd\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import spacy\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.rslp import RSLPStemmer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.utils import plot_model\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "import pydot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sb.set_theme(context='notebook', style='whitegrid')\n",
        "\n",
        "nlp = spacy.load('pt_core_news_md')\n",
        "rslp = RSLPStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(tf.__version__)\n",
        "#print(keras.__version__)\n",
        "print(tf.test.is_built_with_cuda())\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Funções externas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# função de lematização completa do documento\n",
        "def stemmer_text(frase):\n",
        "  tokens = [rslp.stem(w) for w in frase.split()]\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "# função de lematização completa do documento\n",
        "def lemmatizer_text(frase):\n",
        "  doc = nlp(frase)\n",
        "  tokens = [w.lemma_ for w in doc]\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "# função de lematização para os verbos do documento\n",
        "def lemmatizer_verbs(frase):\n",
        "  doc = nlp(frase)\n",
        "  tokens = [w.lemma_ if w.pos_ == 'VERB' else w.text for w in doc]\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "# função de lematização apenas ADV e ADJ\n",
        "def lemmatizer_adv_adj(frase):\n",
        "  doc = nlp(frase)\n",
        "  tokens = [w.lemma_ for w in doc if (w.pos_ == 'ADV' or w.pos_ == 'ADJ')]\n",
        "  return \" \".join(tokens)  \n",
        "\n",
        "def plural_singular(palavra):\n",
        "  r2 = r\"ses$|zes$|res$\"\n",
        "  r3 = r\"ões$|ães$\"\n",
        "  r4 = r\"ais$|éis$|óis$|uis$\"\n",
        "  r5 = r\"is$\"\n",
        "  r6 = r\"eis$\"\n",
        "  if palavra.endswith('s'):\n",
        "    if re.findall(r2, palavra):\n",
        "      return palavra[:-2]\n",
        "    if re.findall(r3, palavra):\n",
        "      return palavra[:-3] + \"ão\"\n",
        "    if re.findall(r4, palavra):\n",
        "      return palavra[:-2] + \"l\"\n",
        "    if re.findall(r5, palavra):\n",
        "      return palavra[:-1] + \"l\"\n",
        "    if re.findall(r6, palavra):\n",
        "      return palavra[:-1] + \"il\"\n",
        "    if palavra.endswith('ns'):\n",
        "      return palavra[:-2] + \"m\"\n",
        "    return palavra[:-1]\n",
        "  return palavra  \n",
        "\n",
        "def pre(frase):\n",
        "  regex = r\"[`,.?:;!&\\\"]\"\n",
        "  palavras = frase.split()\n",
        "  palavras = [p.lower() for p in palavras if p not in stops]\n",
        "  palavras = [re.sub(regex, \"\", p) for p in palavras]\n",
        "  palavras = [p for p in palavras if len(p) >= 3]\n",
        "  palavras = [p for p in palavras if not p.isnumeric()]\n",
        "  palavras = [plural_singular(p) for p in palavras]\n",
        "\n",
        "  return \" \".join(palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = \"Este fime é muito bom e especial\"\n",
        "\n",
        "d = nlp(t)\n",
        "for w in d:\n",
        "    print(w.text, w.pos_, w.lemma_, w.dep_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Carregando os Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMBI8SQtps1n"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./data/reviews-pt-br.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s__lBzDQwrcG",
        "outputId": "0bd3d84d-be60-4da3-c598-62f4a045b6c6"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue0nV0uVo3OZ"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyKC9Vhkp0BK"
      },
      "source": [
        "## Conferindo se temos dados nulos ou duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nze8UbKhosm9"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[df.duplicated()].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distribuição das respostas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.groupby('sentimento').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# em percentual\n",
        "round(df.groupby('sentimento').count().texto / df.shape[0] * 100, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FziwgqJmw9OD"
      },
      "outputs": [],
      "source": [
        "sb.countplot(x=df.sentimento)"
      ]
    },
    {
      "source": [
        "## Criando o conjunto de stopwords (NLTK + SPACY)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stopwords_nltk = nltk.corpus.stopwords.words('portuguese')\n",
        "stopwords_spacy = nlp.Defaults.stop_words\n",
        "stops = list(set(stopwords_spacy).union(stopwords_nltk))\n",
        "print(sorted(stops))"
      ]
    },
    {
      "source": [
        "# Analisando o conjunto total das palavras"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# exemplo de críticas no nosso dataframe\n",
        "print(df.texto[0])\n",
        "print(\"-\" * 40)\n",
        "print(df.texto[1536])\n",
        "print(\"-\" * 40)\n",
        "print(df.texto[8192])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# criando um texto completo com todas as críticas\n",
        "texto_completo = \" \".join([texto for texto in df.texto])\n",
        "texto_completo_pos = \" \".join([texto for texto in df[df.sentimento == 'pos'].texto])\n",
        "texto_completo_neg = \" \".join([texto for texto in df[df.sentimento == 'neg'].texto])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"QTD de todas as palvaras\", len(texto_completo.split()))\n",
        "print(\"QTD de todas as palvaras das críticas positivas\", len(texto_completo_pos.split()))\n",
        "print(\"QTD de todas as palvaras das críticas negativas\", len(texto_completo_neg.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# criando uma wordcloud de todas as palavra sem as stopwords\n",
        "wordcloud = wd.WordCloud(width = 3000, height = 2000, max_words=50, random_state=42, background_color='black', colormap='Blues', collocations=False, stopwords = stops).generate(texto_completo)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.title(\"Todas as palavras da Base de Dados retirando as StopWords\", fontdict={'fontsize':24})\n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\");\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# criando uma wordcloud de todas as palavra sem as stopwords\n",
        "wordcloud = wd.WordCloud(width = 3000, height = 2000, max_words=50, random_state=42, background_color='black', colormap='Blues', collocations=False, stopwords = stops).generate(texto_completo_pos)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.title(\"Todas as palavras POSITIVAS da Base de Dados retirando as StopWords\", fontdict={'fontsize':24})\n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\");\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# criando uma wordcloud de todas as palavra sem as stopwords\n",
        "wordcloud = wd.WordCloud(width = 3000, height = 2000, max_words=50, random_state=42, background_color='black', colormap='Blues', collocations=False, stopwords = stops).generate(texto_completo_neg)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.title(\"Todas as palavras NEGATIVAS da Base de Dados retirando as StopWords\", fontdict={'fontsize':24})\n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\");\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "## Criando uma função de pré processamento"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "palavras = texto_completo.split()\n",
        "len(palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# retirando as stopwords das palavras\n",
        "palavras = [p.lower() for p in palavras if p not in stops]\n",
        "len(palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# retirando pontuações das palavras\n",
        "regex = r\"[`,.?:;!&\\\"]\"\n",
        "palavras = [re.sub(regex, \"\", p) for p in palavras]\n",
        "len(palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#retirando palavras menores que 3 caracteres\n",
        "palavras = [p for p in palavras if len(p) >= 3]\n",
        "len(palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# retirando os números das palavras\n",
        "palavras = [p for p in palavras if not p.isnumeric()]\n",
        "len(palavras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#convertendo as palavras para o singular\n",
        "palavras = [plural_singular(p) for p in palavras]\n",
        "len(palavras)"
      ]
    },
    {
      "source": [
        "## Criamos uma função chamada \"pre\" que iremos usar em nossas análises dos modelos"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# Vamos analisar quais palavras são comuns as críticas negativas e positivas"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "todas_palavras_pos = texto_completo_pos.split()\n",
        "todas_palavras_pos = [pre(w) for w in todas_palavras_pos]\n",
        "todas_palavras_pos = [w for w in todas_palavras_pos if w != '']\n",
        "ctpp = Counter()\n",
        "ctpp.update(todas_palavras_pos)\n",
        "ctpp.most_common(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "todas_palavras_neg = texto_completo_neg.split()\n",
        "todas_palavras_neg = [pre(w) for w in todas_palavras_neg]\n",
        "todas_palavras_neg = [w for w in todas_palavras_neg if w != '']\n",
        "ctpn = Counter()\n",
        "ctpn.update(todas_palavras_neg)\n",
        "ctpn.most_common(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "p = ctpp.most_common()\n",
        "n = ctpn.most_common()\n",
        "r = [[x for x in p if x[0] == y[0]] for y in n]\n",
        "r = [x for x in r if len(x) != 0]\n",
        "len(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "palavras_comuns = [w[0][0] for w in r]\n",
        "palavras_comuns[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# criando uma wordcloud das palavras filtradas\n",
        "wordcloud = wd.WordCloud(width = 3000, height = 2000, max_words=50, random_state=42, background_color='black', colormap='Blues', collocations=False).fit_words(dict(ctp))\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.title(\"Todas as palavras da Base de Dados após o Pré Pocessamento\", fontdict={'fontsize':24})\n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\");\n",
        "plt.show()"
      ]
    },
    {
      "source": [
        "## Criando as colunas tratadas"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "#df['stem'] = df.texto.apply(stemmer_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%time\n",
        "#df['lemm'] = df.texto.apply(lemmatizer_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "#df['verb'] = df.texto.apply(lemmatizer_verbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "#df['pre'] = df.texto.apply(pre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df.to_csv(\"./data/criticas.csv\")\n",
        "df = pd.read_csv('../criticas.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stops2 = set(stops).union(palavras_comuns[:100])\n",
        "len(stops2)"
      ]
    },
    {
      "source": [
        "# Dividindo a base em treino e teste"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dividindo com 20% para o treino e random state = 42\n",
        "df_treino, df_teste = train_test_split(\n",
        "      df, \n",
        "      test_size = 0.2, \n",
        "      random_state = 42\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# distribuição das respostas do treino em %\n",
        "round(df_treino.groupby('sentimento').count() / df.shape[0] * 100, 2).texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# distribuição das respostas do teste em %\n",
        "round(df_teste.groupby('sentimento').count() / df.shape[0] * 100, 2).texto"
      ]
    },
    {
      "source": [
        "# Criando o primeiro modelo com árvore de decisão"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "def criar_vetores(base_treino, base_teste, coluna)\n",
        "    nomes = ['CV ngram(1,1)', 'CV ngram(1,2)', 'CV ngram(1,3)', 'CV ngram(2,2)', 'CV ngram(2,3)', 'CV ngram(3,3)',\n",
        "                'TF ngram(1,1)', 'TF ngram(1,2)', 'TF ngram(1,3)', 'TF ngram(2,2)', 'TF ngram(2,3)', 'TF ngram(3,3)',]\n",
        "    vetores[]\n",
        "\n",
        "    for x in range(1,4):\n",
        "        for y in range(1,4): \n",
        "            if (y < x):\n",
        "                continue\n",
        "            vect = CountVectorizer(ngram_range=(x,y), stops_words = stops) \n",
        "\n",
        "vect.fit(df_treino['texto'])\n",
        "vect_treino = vect.transform(df_treino['texto'])\n",
        "vect_teste = vect.transform(df_teste['texto'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas com CountVectorizer\n",
        "vect = CountVectorizer(ngram_range=(1,1)) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas com CountVectorizer e sem stopwords\n",
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas com CountVectorizer e pré processamento\n",
        "vect = CountVectorizer(ngram_range=(1,1)) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.pre)\n",
        "vect_treino = vect.transform(df_treino.pre)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.pre)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas com CountVectorizer, sem stopwords e Stemmização\n",
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.stem)\n",
        "vect_treino = vect.transform(df_treino.stem)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.stem)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas com CountVectorizer, sem stopwords e Lemmatização\n",
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.lemm)\n",
        "vect_treino = vect.transform(df_treino.lemm)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.lemm)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas com CountVectorizer, sem stopwords e Lemmatização de verbos\n",
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.verb)\n",
        "vect_treino = vect.transform(df_treino.verb)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.verb)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "source": [
        "## Vamos continuar a analise com a base pre processada e original sem stop words"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Testando o TFIDF"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas com TfIdf e sem stopwords\n",
        "vect = TfidfVectorizer(ngram_range=(1,1), stop_words=stops) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas com TfIdf e pré processamento\n",
        "vect = TfidfVectorizer(ngram_range=(1,1)) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.pre)\n",
        "vect_treino = vect.transform(df_treino.pre)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.pre)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "source": [
        "## Observamos um melhor resultado com o CountVectorizer"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Testando modelos com Bigramas"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas + Bigramas com CountVectorizer e sem stopwords\n",
        "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em bigramas com CountVectorizer e sem stopwords\n",
        "vect = CountVectorizer(ngram_range=(2,2), stop_words=stops) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas + bigramas com CountVectorizer e pré processamento\n",
        "vect = CountVectorizer(ngram_range=(1,2)) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.pre)\n",
        "vect_treino = vect.transform(df_treino.pre)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.pre)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em bigramas com CountVectorizer e pré processamento\n",
        "vect = CountVectorizer(ngram_range=(2,2)) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.pre)\n",
        "vect_treino = vect.transform(df_treino.pre)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.pre)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "source": [
        "## Testando modelo com Trigramas usando o melhor modelo com Bigramas"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas + Bigramas + Trigramas com CountVectorizer e sem stopwords\n",
        "vect = CountVectorizer(ngram_range=(1,3), stop_words=stops) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "source": [
        "## Como não houve melhora vamos continuar com Unigramas + Bigramas"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas + Bigramas com CountVectorizer e sem stopwords\n",
        "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops)#, min_df=10, max_df=0.95) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas + Bigramas com CountVectorizer e sem stopwords\n",
        "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops, min_df=10) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas + Bigramas com CountVectorizer e sem stopwords\n",
        "vect = TfidfVectorizer(ngram_range=(1,2), stop_words=stops, min_df=10) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas + Bigramas com CountVectorizer e sem stopwords\n",
        "vect = HashingVectorizer(ngram_range=(1,2), stop_words=stops) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)\n",
        "\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "source": [
        "# Random Forest"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas sem stopwords\n",
        "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops, min_df=10) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# treinamento do modelo Árvore Aleatória\n",
        "tree = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas sem stopwords\n",
        "vect = TfidfVectorizer(ngram_range=(1,2), stop_words=stops, min_df=10) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# vetorização em unigramas sem stopwords\n",
        "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops, min_df=10) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino.texto)\n",
        "vect_treino = vect.transform(df_treino.texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = AdaBoostClassifier(n_estimators=200, random_state=42)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = BaggingClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# treinamento do modelo árvore de decisão\n",
        "tree = ExtraTreesClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "tree.fit(vect_treino, df_treino.sentimento)\n",
        "\n",
        "# escoragem da classificação na amostra de teste\n",
        "vect_teste = vect.transform(df_teste.texto)\n",
        "predito = tree.predict(vect_teste)\n",
        "\n",
        "# mensuração do resultado pela acurácia e f1 score\n",
        "accuracy = accuracy_score(df_teste.sentimento, predito)\n",
        "f1score = f1_score(df_teste.sentimento, predito, average='weighted')\n",
        "\n",
        "print(f\"Acuracidade: {round(accuracy,4)}\")\n",
        "print(f\"F1 Score: {round(f1score,4)}\")"
      ]
    },
    {
      "source": [
        "# MLP"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_mlp = df[['texto', 'pre']].copy()\n",
        "df_mlp['target'] = [0 if x == 'pos' else 1 for x in df.sentimento]\n",
        "df_mlp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dividindo com 20% para o treino e random state = 42\n",
        "df_treino_mlp, df_teste_mlp = train_test_split(\n",
        "      df_mlp, \n",
        "      test_size = 0.2, \n",
        "      random_state = 42\n",
        "  )\n",
        "df_mlp.info()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# distribuição das respostas do treino em %\n",
        "round(df_treino_mlp.groupby('target').count() / df_mlp.shape[0] * 100, 2).texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# distribuição das respostas do teste em %\n",
        "round(df_teste_mlp.groupby('target').count() / df_mlp.shape[0] * 100, 2).texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts(df_mlp.pre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t.document_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoded_docs = t.texts_to_matrix(df_mlp.pre, mode='count')\n",
        "encoded_docs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vetorização em unigramas + Bigramas com CountVectorizer e sem stopwords\n",
        "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops, min_df=50) \n",
        "#vect = CountVectorizer(ngram_range=(1,1), stop_words=stops) \n",
        "# utilizando a base completa sem ajustes\n",
        "vect.fit(df_treino_mlp.pre)\n",
        "vect_treino = vect.transform(df_treino_mlp.pre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = vect_treino.toarray()\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = df_treino_mlp.target.values.reshape(-1,1)\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Modelo da Rede MLP\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_dim=X.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "#Processo de treinamento com 80% dos dados\n",
        "history  = model.fit(X, y, epochs=5, validation_split=0.3, batch_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vect_teste = vect.transform(df_teste_mlp.pre)\n",
        "X_teste = vect_teste.toarray()\n",
        "X_teste.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_teste = df_teste_mlp.target.values.reshape(-1,1)\n",
        "y_teste.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Processo de teste com 30% dos dados que não foram utilizados no treinamento\n",
        "y_pred_classes  = model.predict_classes(X_teste)\n",
        "y_pred_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores = model.evaluate(X_teste, y_teste)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68SiMjcWqD_m"
      },
      "source": [
        "#### **Validação do professor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T24EasckqG2I"
      },
      "source": [
        "Consolidar apenas os scripts do seu **modelo campeão**, desde o carregamento do dataframe, separação das amostras, tratamentos utilizados (funções, limpezas, etc.), criação dos objetos de vetorização dos textos e modelo treinado e outras implementações utilizadas no processo de desenvolvimento do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFA-CYfawkEJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TemplateTrabalhoFinal-NLP.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "c24c9dacf042e5cf8b743bae11b2cef3a95983df3bc5153773d9ffef1d5207d2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "metadata": {
      "interpreter": {
        "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}